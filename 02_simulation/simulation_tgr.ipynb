{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Libraries\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import log_hyperu as hyperu\n",
    "import tgr as tgr\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the data sets\n",
    "\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "# Structure of the data sets\n",
    "## Set X_A: Sparse Coefficients with many data points\n",
    "## Set X_B: Dense Coefficients with many data points\n",
    "## Set X_C: Sparse Coefficients with few data points\n",
    "## Set X_D: Dense Coefficients with few data points\n",
    "\n",
    "\n",
    "### Set X_A\n",
    "variables = 10\n",
    "sample = 100\n",
    "true_coefs = torch.tensor([[0],[0],[5.3],[4.2],[0],[0],[6.9],[0],[0],[0]])\n",
    "X = torch.randn(sample, variables)\n",
    "Y = X @ true_coefs + torch.randn(sample, 1) * 0.1\n",
    "#print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1500], Loss: 111.72769165039062\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Lucas Paul\\Documents\\GitHub_Repos\\MScThesis_Econ\\02_simulation\\simulation_tgr.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lucas%20Paul/Documents/GitHub_Repos/MScThesis_Econ/02_simulation/simulation_tgr.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m iterations \u001b[39m=\u001b[39m \u001b[39m1500\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lucas%20Paul/Documents/GitHub_Repos/MScThesis_Econ/02_simulation/simulation_tgr.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m starttime \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Lucas%20Paul/Documents/GitHub_Repos/MScThesis_Econ/02_simulation/simulation_tgr.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m trained_model2, coefs, loss_of_optimization \u001b[39m=\u001b[39m tgr\u001b[39m.\u001b[39;49mTripleGammaModel(X, Y, \u001b[39m1\u001b[39;49m, \u001b[39m0.51\u001b[39;49m, \u001b[39m0.001\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m, num_epochs\u001b[39m=\u001b[39;49miterations) \u001b[39m# Covariates, Targets, Penalty, a, c, kappa, normalization=True\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lucas%20Paul/Documents/GitHub_Repos/MScThesis_Econ/02_simulation/simulation_tgr.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m endtime \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lucas%20Paul/Documents/GitHub_Repos/MScThesis_Econ/02_simulation/simulation_tgr.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mWith \u001b[39m\u001b[39m{\u001b[39;00miterations\u001b[39m}\u001b[39;00m\u001b[39m iterations, TG Regularization took \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mround\u001b[39m(endtime\u001b[39m-\u001b[39mstarttime,\u001b[39m4\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m seconds. On this system, that is roughly \u001b[39m\u001b[39m{\u001b[39;00m(endtime\u001b[39m-\u001b[39mstarttime)\u001b[39m/\u001b[39miterations\u001b[39m}\u001b[39;00m\u001b[39m seconds per iteration.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Lucas Paul\\Documents\\GitHub_Repos\\MScThesis_Econ\\02_simulation\\tgr.py:47\u001b[0m, in \u001b[0;36mTripleGammaModel\u001b[1;34m(X, y, penalty, a, c, kappa, norm, num_epochs, lr)\u001b[0m\n\u001b[0;32m     45\u001b[0m outputs \u001b[39m=\u001b[39m model(X)\n\u001b[0;32m     46\u001b[0m loss \u001b[39m=\u001b[39m TripleGammaReg_Loss(outputs, y, model\u001b[39m.\u001b[39mlinear\u001b[39m.\u001b[39mweight, penalty, a, c, kappa, norm)\n\u001b[1;32m---> 47\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     48\u001b[0m coef_list\u001b[39m.\u001b[39mappend(model\u001b[39m.\u001b[39mlinear\u001b[39m.\u001b[39mweight)\n\u001b[0;32m     49\u001b[0m loss_list\u001b[39m.\u001b[39mappend(loss)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\function.py:289\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    284\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mImplementing both \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbackward\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvjp\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for a custom \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    285\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFunction is not allowed. You should only implement one \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mof them.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    287\u001b[0m     )\n\u001b[0;32m    288\u001b[0m user_fn \u001b[39m=\u001b[39m vjp_fn \u001b[39mif\u001b[39;00m vjp_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Function\u001b[39m.\u001b[39mvjp \u001b[39melse\u001b[39;00m backward_fn\n\u001b[1;32m--> 289\u001b[0m \u001b[39mreturn\u001b[39;00m user_fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[1;32mc:\\Users\\Lucas Paul\\Documents\\GitHub_Repos\\MScThesis_Econ\\02_simulation\\log_hyperu.py:78\u001b[0m, in \u001b[0;36mlog_hyperu.backward\u001b[1;34m(ctx, grad_output)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(ctx, grad_output):\n\u001b[0;32m     77\u001b[0m     a, b, x, u_res \u001b[39m=\u001b[39m ctx\u001b[39m.\u001b[39msaved_tensors\n\u001b[1;32m---> 78\u001b[0m     grad_x \u001b[39m=\u001b[39m grad_output \u001b[39m*\u001b[39m (\u001b[39m-\u001b[39ma \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mdiv(robust_hyperu(a \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, b \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, x), u_res))\n\u001b[0;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, grad_x\n",
      "File \u001b[1;32mc:\\Users\\Lucas Paul\\Documents\\GitHub_Repos\\MScThesis_Econ\\02_simulation\\log_hyperu.py:52\u001b[0m, in \u001b[0;36mrobust_hyperu\u001b[1;34m(a, b, z)\u001b[0m\n\u001b[0;32m     47\u001b[0m res \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(torch\u001b[39m.\u001b[39mlogical_and(small_z, b \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m),\n\u001b[0;32m     48\u001b[0m                     torch\u001b[39m.\u001b[39mexp(torch\u001b[39m.\u001b[39mlgamma(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m b) \u001b[39m-\u001b[39m torch\u001b[39m.\u001b[39mlgamma(a \u001b[39m-\u001b[39m b \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)),\n\u001b[0;32m     49\u001b[0m                     res)\n\u001b[0;32m     51\u001b[0m \u001b[39m# Fill up all where no special case applies\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m res \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(torch\u001b[39m.\u001b[39misnan(res), sp\u001b[39m.\u001b[39;49mhyperu(a, b, z), res)\n\u001b[0;32m     54\u001b[0m \u001b[39mreturn\u001b[39;00m(res)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:1068\u001b[0m, in \u001b[0;36mTensor.__array_wrap__\u001b[1;34m(self, array)\u001b[0m\n\u001b[0;32m   1064\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   1066\u001b[0m \u001b[39m# Wrap Numpy array again in a suitable tensor when done, to support e.g.\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m \u001b[39m# `numpy.sin(tensor) -> tensor` or `numpy.greater(tensor, 0) -> ByteTensor`\u001b[39;00m\n\u001b[1;32m-> 1068\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array_wrap__\u001b[39m(\u001b[39mself\u001b[39m, array):\n\u001b[0;32m   1069\u001b[0m     \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1070\u001b[0m         \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   1071\u001b[0m             Tensor\u001b[39m.\u001b[39m__array_wrap__, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, array\u001b[39m=\u001b[39marray\n\u001b[0;32m   1072\u001b[0m         )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iterations = 1500\n",
    "starttime = time.time()\n",
    "trained_model2, coefs, loss_of_optimization = tgr.TripleGammaModel(X, Y, 1, 0.51, 0.001, 2, True, num_epochs=iterations) # Covariates, Targets, Penalty, a, c, kappa, normalization=True\n",
    "endtime = time.time()\n",
    "print(f'With {iterations} iterations, TG Regularization took {round(endtime-starttime,4)} seconds. On this system, that is roughly {(endtime-starttime)/iterations} seconds per iteration.')\n",
    "coefficients = trained_model2.linear.weight.detach().numpy()\n",
    "#intercept = trained_model2.linear.bias.item()\n",
    "\n",
    "# For LASSO Imitation: (X, Y, 1, 1, 30, 1, True, num_epochs=2000) # Covariates, Targets, Penalty, a, c, kappa, normalization=True\n",
    "\n",
    "#print(\"Coefficients General Function:\", coefficients)\n",
    "#print(\"Intercept General Function:\", intercept)\n",
    "print(coefficients[0].tolist())\n",
    "true_coefs_TGR = coefficients[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 10.6036\n",
      "Epoch [200/1000], Loss: 7.0338\n",
      "Epoch [300/1000], Loss: 6.6343\n",
      "Epoch [400/1000], Loss: 6.5833\n",
      "Epoch [500/1000], Loss: 6.5743\n",
      "Epoch [600/1000], Loss: 6.5747\n",
      "Epoch [700/1000], Loss: 6.5778\n",
      "Epoch [800/1000], Loss: 6.5804\n",
      "Epoch [900/1000], Loss: 6.5821\n",
      "Epoch [1000/1000], Loss: 6.5785\n",
      "With 1000 iterations, Regular LASSO Regularization took 1.3717 seconds. On this system, that is roughly 0.0013716657161712646 seconds per iteration.\n",
      "linear.weight: [[-0.08131898939609528, 0.11880242824554443, 5.130025863647461, 4.099651336669922, 0.0024799692910164595, -0.008265404962003231, 6.229494094848633, -0.03251353278756142, 0.006142516154795885, -0.11067549884319305]]\n"
     ]
    }
   ],
   "source": [
    "#LASSO\n",
    "# Define your model\n",
    "starttime = time.time()\n",
    "class LinearRegressionLasso(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegressionLasso, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = 10\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "lambda_lasso = 1 # L1 regularization parameter\n",
    "\n",
    "# Create the model\n",
    "model = LinearRegressionLasso(input_size, output_size)\n",
    "\n",
    "# Define loss function (MSE loss with L1 regularization)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, Y)\n",
    "\n",
    "    # Total loss with L1 regularization\n",
    "    l1_norm = sum(torch.linalg.norm(p, 1) for p in model.parameters())\n",
    "\n",
    "    loss = loss + lambda_lasso * l1_norm\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "endtime = time.time()\n",
    "print(f'With {num_epochs} iterations, Regular LASSO Regularization took {round(endtime-starttime,4)} seconds. On this system, that is roughly {(endtime-starttime)/num_epochs} seconds per iteration.')\n",
    "\n",
    "# After training, you can access the learned coefficients\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        print(f'{name}: {param.data.tolist()}')\n",
    "        coefs_LASSO = param.data.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CopmuteRMSE(Y, X, coefficients, method):\n",
    "    # Variable Type Check\n",
    "    if not all(isinstance(t, torch.Tensor) for t in [Y, X, coefficients]):\n",
    "        raise ValueError(\"Not all inputs are PyTorch tensors!\")\n",
    "    \n",
    "    Y_hat = X @ coefficients\n",
    "    Y = torch.squeeze(Y)\n",
    "    mse = torch.sum((Y - Y_hat)**2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "    print(method)\n",
    "    #print(Y_hat)\n",
    "    #print(Y)\n",
    "    print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "ols = linear_model.LinearRegression()\n",
    "model = ols.fit(X, Y)\n",
    "ols = model.coef_.tolist()[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True COEFS [0.0, 0.0, 5.300000190734863, 4.199999809265137, 0.0, 0.0, 6.900000095367432, 0.0, 0.0, 0.0]\n",
      "TGR: [0.005852716509252787, -0.003165281843394041, 5.298001289367676, 4.181629180908203, -0.006983057130128145, 2.2076303139328957e-05, 6.898454666137695, -0.008982997387647629, -0.00298516359180212, -0.00657022837549448]\n",
      "LASSO: [-0.08131898939609528, 0.11880242824554443, 5.130025863647461, 4.099651336669922, 0.0024799692910164595, -0.008265404962003231, 6.229494094848633, -0.03251353278756142, 0.006142516154795885, -0.11067549884319305]\n",
      "OLS: [-0.011616242118179798, 0.005331128835678101, 5.29803991317749, 4.181795120239258, -0.013096123933792114, 0.003976382315158844, 6.89629602432251, -0.014410754665732384, -0.0016365605406463146, -0.008706651628017426]\n",
      "\n",
      "\n",
      "TGR\n",
      "tensor(0.9780)\n",
      "LASSO\n",
      "tensor(5.8565)\n",
      "OLS\n",
      "tensor(0.9644)\n"
     ]
    }
   ],
   "source": [
    "print(\"True COEFS\", torch.squeeze(true_coefs).T.tolist())\n",
    "print(\"TGR:\",torch.tensor(true_coefs_TGR).tolist())\n",
    "print(\"LASSO:\",torch.squeeze(torch.tensor(coefs_LASSO)).tolist())\n",
    "print(\"OLS:\", ols)\n",
    "print(\"\\n\")\n",
    "CopmuteRMSE(Y,X,torch.tensor(true_coefs_TGR), \"TGR\")\n",
    "CopmuteRMSE(Y,X,torch.squeeze(torch.tensor(coefs_LASSO)), \"LASSO\")\n",
    "CopmuteRMSE(Y,X,torch.squeeze(torch.tensor(ols)), \"OLS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/500], Loss: 0.3655189573764801\n",
      "Epoch [200/500], Loss: 0.35940754413604736\n",
      "Epoch [300/500], Loss: 0.3588652014732361\n",
      "Epoch [400/500], Loss: 0.3591199815273285\n",
      "Epoch [500/500], Loss: 0.36044442653656006\n",
      "Epoch [100/500], Loss: 0.35954350233078003\n",
      "Epoch [200/500], Loss: 0.36068010330200195\n",
      "Epoch [300/500], Loss: 0.35909420251846313\n",
      "Epoch [400/500], Loss: 0.3589637875556946\n",
      "Epoch [500/500], Loss: 0.3584367036819458\n",
      "Epoch [100/500], Loss: 0.3587866425514221\n",
      "Epoch [200/500], Loss: 0.35888543725013733\n",
      "Epoch [300/500], Loss: 0.3582509160041809\n",
      "Epoch [400/500], Loss: 0.3582497239112854\n",
      "Epoch [500/500], Loss: 0.3593846559524536\n",
      "Epoch [100/500], Loss: 0.359093576669693\n",
      "Epoch [200/500], Loss: 0.36009037494659424\n",
      "Epoch [300/500], Loss: 0.3578297793865204\n",
      "Epoch [400/500], Loss: 0.3591449558734894\n",
      "Epoch [500/500], Loss: 0.35863155126571655\n",
      "Epoch [100/500], Loss: 0.3580813705921173\n",
      "Epoch [200/500], Loss: 0.35943007469177246\n",
      "Epoch [300/500], Loss: 0.3585241436958313\n",
      "Epoch [400/500], Loss: 0.35862869024276733\n",
      "Epoch [500/500], Loss: 0.3596237301826477\n",
      "Epoch [100/500], Loss: 0.36002808809280396\n",
      "Epoch [200/500], Loss: 0.35903555154800415\n",
      "Epoch [300/500], Loss: 0.35900071263313293\n",
      "Epoch [400/500], Loss: 0.35813072323799133\n",
      "Epoch [500/500], Loss: 0.359984427690506\n",
      "Epoch [100/500], Loss: 0.35971492528915405\n",
      "Epoch [200/500], Loss: 0.35881519317626953\n",
      "Epoch [300/500], Loss: 0.3590283989906311\n",
      "Epoch [400/500], Loss: 0.35965028405189514\n",
      "Epoch [500/500], Loss: 0.35913535952568054\n",
      "Epoch [100/500], Loss: 0.35970062017440796\n",
      "Epoch [200/500], Loss: 0.35958272218704224\n",
      "Epoch [300/500], Loss: 0.3596659302711487\n",
      "Epoch [400/500], Loss: 0.3591518700122833\n",
      "Epoch [500/500], Loss: 0.3576345145702362\n",
      "Epoch [100/500], Loss: 0.3596034049987793\n",
      "Epoch [200/500], Loss: 0.3583010733127594\n",
      "Epoch [300/500], Loss: 0.3582216799259186\n",
      "Epoch [400/500], Loss: 0.3591917157173157\n",
      "Epoch [500/500], Loss: 0.3597338795661926\n",
      "Epoch [100/500], Loss: 0.36001116037368774\n",
      "Epoch [200/500], Loss: 0.35865887999534607\n",
      "Epoch [300/500], Loss: 0.3594025671482086\n",
      "Epoch [400/500], Loss: 0.3596420884132385\n",
      "Epoch [500/500], Loss: 0.35997357964515686\n",
      "Epoch [100/500], Loss: 0.3600420653820038\n",
      "Epoch [200/500], Loss: 0.3582417666912079\n",
      "Epoch [300/500], Loss: 0.35959020256996155\n",
      "Epoch [400/500], Loss: 0.35931339859962463\n",
      "Epoch [500/500], Loss: 0.35785913467407227\n",
      "Epoch [100/500], Loss: 0.3592507243156433\n",
      "Epoch [200/500], Loss: 0.358537495136261\n",
      "Epoch [300/500], Loss: 0.3594425320625305\n",
      "Epoch [400/500], Loss: 0.3595654368400574\n",
      "Epoch [500/500], Loss: 0.35833296179771423\n",
      "Epoch [100/500], Loss: 0.35995522141456604\n",
      "Epoch [200/500], Loss: 0.3581448197364807\n",
      "Epoch [300/500], Loss: 0.3582340478897095\n",
      "Epoch [400/500], Loss: 0.3593200445175171\n",
      "Epoch [500/500], Loss: 0.35938435792922974\n",
      "Epoch [100/500], Loss: 0.35956496000289917\n",
      "Epoch [200/500], Loss: 0.3593953251838684\n",
      "Epoch [300/500], Loss: 0.3594515323638916\n",
      "Epoch [400/500], Loss: 0.3582642674446106\n",
      "Epoch [500/500], Loss: 0.3593803942203522\n",
      "Epoch [100/500], Loss: 0.35925665497779846\n",
      "Epoch [200/500], Loss: 0.35888051986694336\n",
      "Epoch [300/500], Loss: 0.358691930770874\n",
      "Epoch [400/500], Loss: 0.3581659197807312\n",
      "Epoch [500/500], Loss: 0.3585982322692871\n",
      "Epoch [100/500], Loss: 0.35939496755599976\n",
      "Epoch [200/500], Loss: 0.3583585023880005\n",
      "Epoch [300/500], Loss: 0.3588210940361023\n",
      "Epoch [400/500], Loss: 0.357401043176651\n",
      "Epoch [500/500], Loss: 0.35973507165908813\n",
      "Epoch [100/500], Loss: 0.357974648475647\n",
      "Epoch [200/500], Loss: 0.358428031206131\n",
      "Epoch [300/500], Loss: 0.3594851791858673\n",
      "Epoch [400/500], Loss: 0.35756829380989075\n",
      "Epoch [500/500], Loss: 0.35902756452560425\n",
      "Epoch [100/500], Loss: 0.35937273502349854\n",
      "Epoch [200/500], Loss: 0.3584194779396057\n",
      "Epoch [300/500], Loss: 0.35916462540626526\n",
      "Epoch [400/500], Loss: 0.35904034972190857\n",
      "Epoch [500/500], Loss: 0.35891374945640564\n",
      "Epoch [100/500], Loss: 0.35949188470840454\n",
      "Epoch [200/500], Loss: 0.36022937297821045\n",
      "Epoch [300/500], Loss: 0.35961848497390747\n",
      "Epoch [400/500], Loss: 0.3597978949546814\n",
      "Epoch [500/500], Loss: 0.35881632566452026\n",
      "Epoch [100/500], Loss: 0.35851576924324036\n",
      "Epoch [200/500], Loss: 0.3599616587162018\n",
      "Epoch [300/500], Loss: 0.35944539308547974\n",
      "Epoch [400/500], Loss: 0.3582906424999237\n",
      "Epoch [500/500], Loss: 0.3596101701259613\n",
      "Epoch [100/500], Loss: 0.3582250773906708\n",
      "Epoch [200/500], Loss: 0.35816359519958496\n",
      "Epoch [300/500], Loss: 0.3593252897262573\n",
      "Epoch [400/500], Loss: 0.35967105627059937\n",
      "Epoch [500/500], Loss: 0.35903000831604004\n",
      "Epoch [100/500], Loss: 0.3599294424057007\n",
      "Epoch [200/500], Loss: 0.3591406047344208\n",
      "Epoch [300/500], Loss: 0.3593931496143341\n",
      "Epoch [400/500], Loss: 0.35920485854148865\n",
      "Epoch [500/500], Loss: 0.35789722204208374\n",
      "Epoch [100/500], Loss: 0.35800901055336\n",
      "Epoch [200/500], Loss: 0.36089783906936646\n",
      "Epoch [300/500], Loss: 0.35838696360588074\n",
      "Epoch [400/500], Loss: 0.35936540365219116\n",
      "Epoch [500/500], Loss: 0.35779350996017456\n",
      "Epoch [100/500], Loss: 0.3587823212146759\n",
      "Epoch [200/500], Loss: 0.3590947389602661\n",
      "Epoch [300/500], Loss: 0.35973039269447327\n",
      "Epoch [400/500], Loss: 0.35871198773384094\n",
      "Epoch [500/500], Loss: 0.3590323328971863\n",
      "Epoch [100/500], Loss: 0.3594236969947815\n",
      "Epoch [200/500], Loss: 0.35848066210746765\n",
      "Epoch [300/500], Loss: 0.35998815298080444\n",
      "Epoch [400/500], Loss: 0.3582516610622406\n",
      "Epoch [500/500], Loss: 0.3596077561378479\n",
      "Epoch [100/500], Loss: 0.3596772253513336\n",
      "Epoch [200/500], Loss: 0.3588206171989441\n",
      "Epoch [300/500], Loss: 0.3597121238708496\n",
      "Epoch [400/500], Loss: 0.3579965829849243\n",
      "Epoch [500/500], Loss: 0.35828128457069397\n",
      "Epoch [100/500], Loss: 0.3585779070854187\n",
      "Epoch [200/500], Loss: 0.3589176833629608\n",
      "Epoch [300/500], Loss: 0.359904021024704\n",
      "Epoch [400/500], Loss: 0.3591305911540985\n",
      "Epoch [500/500], Loss: 0.35864824056625366\n",
      "Epoch [100/500], Loss: 0.35890814661979675\n",
      "Epoch [200/500], Loss: 0.35888025164604187\n",
      "Epoch [300/500], Loss: 0.3590995669364929\n",
      "Epoch [400/500], Loss: 0.35835200548171997\n",
      "Epoch [500/500], Loss: 0.3585715889930725\n",
      "Epoch [100/500], Loss: 0.35913926362991333\n",
      "Epoch [200/500], Loss: 0.3593236804008484\n",
      "Epoch [300/500], Loss: 0.358703076839447\n",
      "Epoch [400/500], Loss: 0.3592446744441986\n",
      "Epoch [500/500], Loss: 0.35841870307922363\n",
      "Epoch [100/500], Loss: 0.35774391889572144\n",
      "Epoch [200/500], Loss: 0.35810983180999756\n",
      "Epoch [300/500], Loss: 0.3579460382461548\n",
      "Epoch [400/500], Loss: 0.35917845368385315\n",
      "Epoch [500/500], Loss: 0.35897162556648254\n",
      "Epoch [100/500], Loss: 0.3587348461151123\n",
      "Epoch [200/500], Loss: 0.3602176606655121\n",
      "Epoch [300/500], Loss: 0.36002790927886963\n",
      "Epoch [400/500], Loss: 0.35906708240509033\n",
      "Epoch [500/500], Loss: 0.3604656159877777\n",
      "Epoch [100/500], Loss: 0.3589051365852356\n",
      "Epoch [200/500], Loss: 0.35999801754951477\n",
      "Epoch [300/500], Loss: 0.3591378331184387\n",
      "Epoch [400/500], Loss: 0.35915836691856384\n",
      "Epoch [500/500], Loss: 0.3583512008190155\n",
      "Epoch [100/500], Loss: 0.3585684895515442\n",
      "Epoch [200/500], Loss: 0.3587007522583008\n",
      "Epoch [300/500], Loss: 0.3596073091030121\n",
      "Epoch [400/500], Loss: 0.35847166180610657\n",
      "Epoch [500/500], Loss: 0.3589359223842621\n",
      "Epoch [100/500], Loss: 0.3581894338130951\n",
      "Epoch [200/500], Loss: 0.3582994341850281\n",
      "Epoch [300/500], Loss: 0.3582293391227722\n",
      "Epoch [400/500], Loss: 0.3586244583129883\n",
      "Epoch [500/500], Loss: 0.3600199222564697\n",
      "Epoch [100/500], Loss: 0.35969895124435425\n",
      "Epoch [200/500], Loss: 0.359037309885025\n",
      "Epoch [300/500], Loss: 0.3584033250808716\n",
      "Epoch [400/500], Loss: 0.3591066300868988\n",
      "Epoch [500/500], Loss: 0.3594478964805603\n",
      "Epoch [100/500], Loss: 0.3594191074371338\n",
      "Epoch [200/500], Loss: 0.3579697012901306\n",
      "Epoch [300/500], Loss: 0.3598025441169739\n",
      "Epoch [400/500], Loss: 0.35946935415267944\n",
      "Epoch [500/500], Loss: 0.35885199904441833\n",
      "Epoch [100/500], Loss: 0.3580916225910187\n",
      "Epoch [200/500], Loss: 0.3591564893722534\n",
      "Epoch [300/500], Loss: 0.35981303453445435\n",
      "Epoch [400/500], Loss: 0.3603121340274811\n",
      "Epoch [500/500], Loss: 0.3592705726623535\n",
      "Epoch [100/500], Loss: 0.3592592775821686\n",
      "Epoch [200/500], Loss: 0.36026960611343384\n",
      "Epoch [300/500], Loss: 0.35891732573509216\n",
      "Epoch [400/500], Loss: 0.3584407866001129\n",
      "Epoch [500/500], Loss: 0.36063361167907715\n",
      "Epoch [100/500], Loss: 0.35993844270706177\n",
      "Epoch [200/500], Loss: 0.3602704703807831\n",
      "Epoch [300/500], Loss: 0.3586125671863556\n",
      "Epoch [400/500], Loss: 0.36012399196624756\n",
      "Epoch [500/500], Loss: 0.35816338658332825\n",
      "Epoch [100/500], Loss: 0.35870417952537537\n",
      "Epoch [200/500], Loss: 0.3594265878200531\n",
      "Epoch [300/500], Loss: 0.3579661548137665\n",
      "Epoch [400/500], Loss: 0.3592117428779602\n",
      "Epoch [500/500], Loss: 0.36016085743904114\n",
      "Epoch [100/500], Loss: 0.3592870831489563\n",
      "Epoch [200/500], Loss: 0.35808542370796204\n",
      "Epoch [300/500], Loss: 0.3603147566318512\n",
      "Epoch [400/500], Loss: 0.36051663756370544\n",
      "Epoch [500/500], Loss: 0.3599340617656708\n",
      "Epoch [100/500], Loss: 0.3595038056373596\n",
      "Epoch [200/500], Loss: 0.35872402787208557\n",
      "Epoch [300/500], Loss: 0.36022475361824036\n",
      "Epoch [400/500], Loss: 0.35884419083595276\n",
      "Epoch [500/500], Loss: 0.35884785652160645\n",
      "Epoch [100/500], Loss: 0.3599950671195984\n",
      "Epoch [200/500], Loss: 0.359331339597702\n",
      "Epoch [300/500], Loss: 0.3584589660167694\n",
      "Epoch [400/500], Loss: 0.35889875888824463\n",
      "Epoch [500/500], Loss: 0.35922369360923767\n",
      "Epoch [100/500], Loss: 0.3595879077911377\n",
      "Epoch [200/500], Loss: 0.3593994081020355\n",
      "Epoch [300/500], Loss: 0.3579593896865845\n",
      "Epoch [400/500], Loss: 0.35966530442237854\n",
      "Epoch [500/500], Loss: 0.35863354802131653\n",
      "Epoch [100/500], Loss: 0.35984188318252563\n",
      "Epoch [200/500], Loss: 0.3588373064994812\n",
      "Epoch [300/500], Loss: 0.35882365703582764\n",
      "Epoch [400/500], Loss: 0.358246386051178\n",
      "Epoch [500/500], Loss: 0.359536737203598\n",
      "Epoch [100/500], Loss: 0.3583625555038452\n",
      "Epoch [200/500], Loss: 0.3587530553340912\n",
      "Epoch [300/500], Loss: 0.3594822883605957\n",
      "Epoch [400/500], Loss: 0.35924670100212097\n",
      "Epoch [500/500], Loss: 0.35906243324279785\n",
      "Epoch [100/500], Loss: 0.3591277301311493\n",
      "Epoch [200/500], Loss: 0.35844987630844116\n",
      "Epoch [300/500], Loss: 0.35809963941574097\n",
      "Epoch [400/500], Loss: 0.3579959273338318\n",
      "Epoch [500/500], Loss: 0.35903483629226685\n",
      "Epoch [100/500], Loss: 0.35938310623168945\n",
      "Epoch [200/500], Loss: 0.3590834438800812\n",
      "Epoch [300/500], Loss: 0.35838478803634644\n",
      "Epoch [400/500], Loss: 0.35878926515579224\n",
      "Epoch [500/500], Loss: 0.35928261280059814\n",
      "Epoch [100/500], Loss: 0.3594161570072174\n",
      "Epoch [200/500], Loss: 0.35875293612480164\n",
      "Epoch [300/500], Loss: 0.35811948776245117\n",
      "Epoch [400/500], Loss: 0.3584744334220886\n",
      "Epoch [500/500], Loss: 0.3588847219944\n",
      "Epoch [100/500], Loss: 0.360379159450531\n",
      "Epoch [200/500], Loss: 0.35878118872642517\n",
      "Epoch [300/500], Loss: 0.3600618243217468\n",
      "Epoch [400/500], Loss: 0.3587054908275604\n",
      "Epoch [500/500], Loss: 0.3597272038459778\n",
      "Epoch [100/500], Loss: 0.3596014380455017\n",
      "Epoch [200/500], Loss: 0.36062997579574585\n",
      "Epoch [300/500], Loss: 0.3581746220588684\n",
      "Epoch [400/500], Loss: 0.35994237661361694\n",
      "Epoch [500/500], Loss: 0.3583470582962036\n",
      "Epoch [100/500], Loss: 0.35935547947883606\n",
      "Epoch [200/500], Loss: 0.358438640832901\n",
      "Epoch [300/500], Loss: 0.36056309938430786\n",
      "Epoch [400/500], Loss: 0.3586861789226532\n",
      "Epoch [500/500], Loss: 0.35929176211357117\n",
      "Epoch [100/500], Loss: 0.35834771394729614\n",
      "Epoch [200/500], Loss: 0.3589599132537842\n",
      "Epoch [300/500], Loss: 0.3600096106529236\n",
      "Epoch [400/500], Loss: 0.3594217896461487\n",
      "Epoch [500/500], Loss: 0.35866281390190125\n",
      "Epoch [100/500], Loss: 0.36065322160720825\n",
      "Epoch [200/500], Loss: 0.3582731783390045\n",
      "Epoch [300/500], Loss: 0.35903945565223694\n",
      "Epoch [400/500], Loss: 0.35919660329818726\n",
      "Epoch [500/500], Loss: 0.3586391508579254\n",
      "Epoch [100/500], Loss: 0.3581283986568451\n",
      "Epoch [200/500], Loss: 0.3583534061908722\n",
      "Epoch [300/500], Loss: 0.35914286971092224\n",
      "Epoch [400/500], Loss: 0.3585037589073181\n",
      "Epoch [500/500], Loss: 0.3591800034046173\n",
      "Epoch [100/500], Loss: 0.35908105969429016\n",
      "Epoch [200/500], Loss: 0.3587736487388611\n",
      "Epoch [300/500], Loss: 0.3599414825439453\n",
      "Epoch [400/500], Loss: 0.359176367521286\n",
      "Epoch [500/500], Loss: 0.3595189154148102\n",
      "Epoch [100/500], Loss: 0.3589824140071869\n",
      "Epoch [200/500], Loss: 0.35838955640792847\n",
      "Epoch [300/500], Loss: 0.36017435789108276\n",
      "Epoch [400/500], Loss: 0.35990816354751587\n",
      "Epoch [500/500], Loss: 0.35877203941345215\n",
      "Epoch [100/500], Loss: 0.35916510224342346\n",
      "Epoch [200/500], Loss: 0.3586069345474243\n",
      "Epoch [300/500], Loss: 0.3589331805706024\n",
      "Epoch [400/500], Loss: 0.3598984479904175\n",
      "Epoch [500/500], Loss: 0.358978271484375\n",
      "Epoch [100/500], Loss: 0.3593393564224243\n",
      "Epoch [200/500], Loss: 0.3600071370601654\n",
      "Epoch [300/500], Loss: 0.357524573802948\n",
      "Epoch [400/500], Loss: 0.359132319688797\n",
      "Epoch [500/500], Loss: 0.359568327665329\n",
      "Epoch [100/500], Loss: 0.3590848743915558\n",
      "Epoch [200/500], Loss: 0.3584365248680115\n",
      "Epoch [300/500], Loss: 0.35910147428512573\n",
      "Epoch [400/500], Loss: 0.35985273122787476\n",
      "Epoch [500/500], Loss: 0.3595445454120636\n",
      "Epoch [100/500], Loss: 0.3583952784538269\n",
      "Epoch [200/500], Loss: 0.358456552028656\n",
      "Epoch [300/500], Loss: 0.35979098081588745\n",
      "Epoch [400/500], Loss: 0.35974058508872986\n",
      "Epoch [500/500], Loss: 0.3587576746940613\n",
      "Epoch [100/500], Loss: 0.35994330048561096\n",
      "Epoch [200/500], Loss: 0.3586530089378357\n",
      "Epoch [300/500], Loss: 0.35912278294563293\n",
      "Epoch [400/500], Loss: 0.359668105840683\n",
      "Epoch [500/500], Loss: 0.35797223448753357\n",
      "Epoch [100/500], Loss: 0.3583349585533142\n",
      "Epoch [200/500], Loss: 0.35798975825309753\n",
      "Epoch [300/500], Loss: 0.35964635014533997\n",
      "Epoch [400/500], Loss: 0.3592539131641388\n",
      "Epoch [500/500], Loss: 0.3587642014026642\n",
      "Epoch [100/500], Loss: 0.3589168190956116\n",
      "Epoch [200/500], Loss: 0.3582783341407776\n",
      "Epoch [300/500], Loss: 0.35981041193008423\n",
      "Epoch [400/500], Loss: 0.3595607876777649\n",
      "Epoch [500/500], Loss: 0.3587350845336914\n",
      "Epoch [100/500], Loss: 0.35942548513412476\n",
      "Epoch [200/500], Loss: 0.3603474497795105\n",
      "Epoch [300/500], Loss: 0.35985127091407776\n",
      "Epoch [400/500], Loss: 0.3602462112903595\n",
      "Epoch [500/500], Loss: 0.3590380549430847\n",
      "Epoch [100/500], Loss: 0.3595731556415558\n",
      "Epoch [200/500], Loss: 0.3589603304862976\n",
      "Epoch [300/500], Loss: 0.35943853855133057\n",
      "Epoch [400/500], Loss: 0.3593745827674866\n",
      "Epoch [500/500], Loss: 0.3595801293849945\n",
      "Epoch [100/500], Loss: 0.35831108689308167\n",
      "Epoch [200/500], Loss: 0.3590874969959259\n",
      "Epoch [300/500], Loss: 0.3578038513660431\n",
      "Epoch [400/500], Loss: 0.35894715785980225\n",
      "Epoch [500/500], Loss: 0.3589727282524109\n",
      "Epoch [100/500], Loss: 0.3585786521434784\n",
      "Epoch [200/500], Loss: 0.35930588841438293\n",
      "Epoch [300/500], Loss: 0.35966306924819946\n",
      "Epoch [400/500], Loss: 0.35980507731437683\n",
      "Epoch [500/500], Loss: 0.35897472500801086\n",
      "Epoch [100/500], Loss: 0.36019280552864075\n",
      "Epoch [200/500], Loss: 0.35818320512771606\n",
      "Epoch [300/500], Loss: 0.35968002676963806\n",
      "Epoch [400/500], Loss: 0.3593388497829437\n",
      "Epoch [500/500], Loss: 0.3587530851364136\n",
      "Epoch [100/500], Loss: 0.35970133543014526\n",
      "Epoch [200/500], Loss: 0.3592087924480438\n",
      "Epoch [300/500], Loss: 0.35938242077827454\n",
      "Epoch [400/500], Loss: 0.3588741421699524\n",
      "Epoch [500/500], Loss: 0.3584153652191162\n",
      "Epoch [100/500], Loss: 0.3588849604129791\n",
      "Epoch [200/500], Loss: 0.3594818711280823\n",
      "Epoch [300/500], Loss: 0.3589462637901306\n",
      "Epoch [400/500], Loss: 0.3589097857475281\n",
      "Epoch [500/500], Loss: 0.35958459973335266\n",
      "Epoch [100/500], Loss: 0.35982367396354675\n",
      "Epoch [200/500], Loss: 0.35911238193511963\n",
      "Epoch [300/500], Loss: 0.3593764007091522\n",
      "Epoch [400/500], Loss: 0.35865721106529236\n",
      "Epoch [500/500], Loss: 0.35900330543518066\n",
      "Epoch [100/500], Loss: 0.35766175389289856\n",
      "Epoch [200/500], Loss: 0.35888946056365967\n",
      "Epoch [300/500], Loss: 0.35993918776512146\n",
      "Epoch [400/500], Loss: 0.35888639092445374\n",
      "Epoch [500/500], Loss: 0.3583022654056549\n",
      "Epoch [100/500], Loss: 0.35902324318885803\n",
      "Epoch [200/500], Loss: 0.35917630791664124\n",
      "Epoch [300/500], Loss: 0.35821104049682617\n",
      "Epoch [400/500], Loss: 0.35910484194755554\n",
      "Epoch [500/500], Loss: 0.3591786026954651\n",
      "Epoch [100/500], Loss: 0.35917896032333374\n",
      "Epoch [200/500], Loss: 0.35839301347732544\n",
      "Epoch [300/500], Loss: 0.36041325330734253\n",
      "Epoch [400/500], Loss: 0.3586180806159973\n",
      "Epoch [500/500], Loss: 0.35833102464675903\n",
      "Epoch [100/500], Loss: 0.3585886061191559\n",
      "Epoch [200/500], Loss: 0.3594994843006134\n",
      "Epoch [300/500], Loss: 0.3584159016609192\n",
      "Epoch [400/500], Loss: 0.35947147011756897\n",
      "Epoch [500/500], Loss: 0.35874372720718384\n",
      "Epoch [100/500], Loss: 0.358112633228302\n",
      "Epoch [200/500], Loss: 0.3601839244365692\n",
      "Epoch [300/500], Loss: 0.35959699749946594\n",
      "Epoch [400/500], Loss: 0.35861021280288696\n",
      "Epoch [500/500], Loss: 0.3585609793663025\n",
      "Epoch [100/500], Loss: 0.3588441014289856\n",
      "Epoch [200/500], Loss: 0.3579636216163635\n",
      "Epoch [300/500], Loss: 0.3602167069911957\n",
      "Epoch [400/500], Loss: 0.35982805490493774\n",
      "Epoch [500/500], Loss: 0.3594782054424286\n",
      "Epoch [100/500], Loss: 0.3585098385810852\n",
      "Epoch [200/500], Loss: 0.3589649200439453\n",
      "Epoch [300/500], Loss: 0.35992997884750366\n",
      "Epoch [400/500], Loss: 0.35854724049568176\n",
      "Epoch [500/500], Loss: 0.3578872084617615\n",
      "Epoch [100/500], Loss: 0.36024996638298035\n",
      "Epoch [200/500], Loss: 0.35887211561203003\n",
      "Epoch [300/500], Loss: 0.35920554399490356\n",
      "Epoch [400/500], Loss: 0.3600383698940277\n",
      "Epoch [500/500], Loss: 0.358247309923172\n",
      "Epoch [100/500], Loss: 0.35969728231430054\n",
      "Epoch [200/500], Loss: 0.3593321144580841\n",
      "Epoch [300/500], Loss: 0.35881003737449646\n",
      "Epoch [400/500], Loss: 0.35834550857543945\n",
      "Epoch [500/500], Loss: 0.35921210050582886\n",
      "Epoch [100/500], Loss: 0.3597884774208069\n",
      "Epoch [200/500], Loss: 0.35910603404045105\n",
      "Epoch [300/500], Loss: 0.3587055504322052\n",
      "Epoch [400/500], Loss: 0.358825147151947\n",
      "Epoch [500/500], Loss: 0.3591853976249695\n",
      "Epoch [100/500], Loss: 0.35869520902633667\n",
      "Epoch [200/500], Loss: 0.35938817262649536\n",
      "Epoch [300/500], Loss: 0.35767993330955505\n",
      "Epoch [400/500], Loss: 0.3592730760574341\n",
      "Epoch [500/500], Loss: 0.3593936860561371\n",
      "Epoch [100/500], Loss: 0.35868385434150696\n",
      "Epoch [200/500], Loss: 0.35907554626464844\n",
      "Epoch [300/500], Loss: 0.35924866795539856\n",
      "Epoch [400/500], Loss: 0.3589089512825012\n",
      "Epoch [500/500], Loss: 0.3578125536441803\n",
      "Epoch [100/500], Loss: 0.35930559039115906\n",
      "Epoch [200/500], Loss: 0.359088271856308\n",
      "Epoch [300/500], Loss: 0.3589457869529724\n",
      "Epoch [400/500], Loss: 0.35893508791923523\n",
      "Epoch [500/500], Loss: 0.3585337996482849\n",
      "Epoch [100/500], Loss: 0.35748574137687683\n",
      "Epoch [200/500], Loss: 0.359518826007843\n",
      "Epoch [300/500], Loss: 0.3588685095310211\n",
      "Epoch [400/500], Loss: 0.3589356243610382\n",
      "Epoch [500/500], Loss: 0.35999026894569397\n",
      "Epoch [100/500], Loss: 0.3595028519630432\n",
      "Epoch [200/500], Loss: 0.35993480682373047\n",
      "Epoch [300/500], Loss: 0.3587634265422821\n",
      "Epoch [400/500], Loss: 0.3586650788784027\n",
      "Epoch [500/500], Loss: 0.35899707674980164\n",
      "Epoch [100/500], Loss: 0.35943180322647095\n",
      "Epoch [200/500], Loss: 0.3590123653411865\n",
      "Epoch [300/500], Loss: 0.35916146636009216\n",
      "Epoch [400/500], Loss: 0.3590514659881592\n",
      "Epoch [500/500], Loss: 0.3585776090621948\n",
      "Epoch [100/500], Loss: 0.36028963327407837\n",
      "Epoch [200/500], Loss: 0.3592105805873871\n",
      "Epoch [300/500], Loss: 0.35890403389930725\n",
      "Epoch [400/500], Loss: 0.3594370186328888\n",
      "Epoch [500/500], Loss: 0.3595885634422302\n",
      "Epoch [100/500], Loss: 0.35822802782058716\n",
      "Epoch [200/500], Loss: 0.35906344652175903\n",
      "Epoch [300/500], Loss: 0.35904979705810547\n",
      "Epoch [400/500], Loss: 0.3596401810646057\n",
      "Epoch [500/500], Loss: 0.3588607609272003\n",
      "Epoch [100/500], Loss: 0.3600977063179016\n",
      "Epoch [200/500], Loss: 0.35906633734703064\n",
      "Epoch [300/500], Loss: 0.35911205410957336\n",
      "Epoch [400/500], Loss: 0.3586316406726837\n",
      "Epoch [500/500], Loss: 0.3587631285190582\n",
      "Epoch [100/500], Loss: 0.3583027720451355\n",
      "Epoch [200/500], Loss: 0.3581605553627014\n",
      "Epoch [300/500], Loss: 0.35918307304382324\n",
      "Epoch [400/500], Loss: 0.3582811653614044\n",
      "Epoch [500/500], Loss: 0.3582760691642761\n",
      "Epoch [100/500], Loss: 0.3584212064743042\n",
      "Epoch [200/500], Loss: 0.3597559928894043\n",
      "Epoch [300/500], Loss: 0.3581036925315857\n",
      "Epoch [400/500], Loss: 0.35919779539108276\n",
      "Epoch [500/500], Loss: 0.3594631552696228\n",
      "Epoch [100/500], Loss: 0.35935521125793457\n",
      "Epoch [200/500], Loss: 0.359649121761322\n",
      "Epoch [300/500], Loss: 0.3583582043647766\n",
      "Epoch [400/500], Loss: 0.3598114848136902\n",
      "Epoch [500/500], Loss: 0.35848069190979004\n",
      "Epoch [100/500], Loss: 0.35932162404060364\n",
      "Epoch [200/500], Loss: 0.35854941606521606\n",
      "Epoch [300/500], Loss: 0.3597484529018402\n",
      "Epoch [400/500], Loss: 0.35895830392837524\n",
      "Epoch [500/500], Loss: 0.35954123735427856\n",
      "Epoch [100/500], Loss: 0.3581995368003845\n",
      "Epoch [200/500], Loss: 0.3597453832626343\n",
      "Epoch [300/500], Loss: 0.35892820358276367\n",
      "Epoch [400/500], Loss: 0.3584595024585724\n",
      "Epoch [500/500], Loss: 0.3593253493309021\n",
      "Epoch [100/500], Loss: 0.35875391960144043\n",
      "Epoch [200/500], Loss: 0.3585963547229767\n",
      "Epoch [300/500], Loss: 0.35925936698913574\n",
      "Epoch [400/500], Loss: 0.35936373472213745\n",
      "Epoch [500/500], Loss: 0.3580384850502014\n",
      "Epoch [100/500], Loss: 0.3580932319164276\n",
      "Epoch [200/500], Loss: 0.3589925169944763\n",
      "Epoch [300/500], Loss: 0.35809484124183655\n",
      "Epoch [400/500], Loss: 0.35919851064682007\n",
      "Epoch [500/500], Loss: 0.3595569133758545\n",
      "Epoch [100/500], Loss: 0.358732670545578\n",
      "Epoch [200/500], Loss: 0.3591267466545105\n",
      "Epoch [300/500], Loss: 0.3594212532043457\n",
      "Epoch [400/500], Loss: 0.35924065113067627\n",
      "Epoch [500/500], Loss: 0.3593730926513672\n",
      "Epoch [100/500], Loss: 0.3590894341468811\n",
      "Epoch [200/500], Loss: 0.35825830698013306\n",
      "Epoch [300/500], Loss: 0.35923710465431213\n",
      "Epoch [400/500], Loss: 0.35773658752441406\n",
      "Epoch [500/500], Loss: 0.3589456379413605\n",
      "Epoch [100/500], Loss: 0.5462871789932251\n",
      "Epoch [200/500], Loss: 0.5462871789932251\n",
      "Epoch [300/500], Loss: 0.5462871789932251\n",
      "Epoch [400/500], Loss: 0.5462871789932251\n",
      "Epoch [500/500], Loss: 0.5462871789932251\n",
      "Epoch [100/500], Loss: 0.5627655982971191\n",
      "Epoch [200/500], Loss: 0.5627655982971191\n",
      "Epoch [300/500], Loss: 0.5627655982971191\n",
      "Epoch [400/500], Loss: 0.5627655982971191\n",
      "Epoch [500/500], Loss: 0.5627655982971191\n",
      "Epoch [100/500], Loss: 0.5792440176010132\n",
      "Epoch [200/500], Loss: 0.5792440176010132\n",
      "Epoch [300/500], Loss: 0.5792440176010132\n",
      "Epoch [400/500], Loss: 0.5792440176010132\n",
      "Epoch [500/500], Loss: 0.5792440176010132\n",
      "Epoch [100/500], Loss: 0.5957224369049072\n",
      "Epoch [200/500], Loss: 0.5957224369049072\n",
      "Epoch [300/500], Loss: 0.5957224369049072\n",
      "Epoch [400/500], Loss: 0.5957224369049072\n",
      "Epoch [500/500], Loss: 0.5957224369049072\n",
      "Epoch [100/500], Loss: 0.6122008562088013\n",
      "Epoch [200/500], Loss: 0.6122008562088013\n",
      "Epoch [300/500], Loss: 0.6122008562088013\n",
      "Epoch [400/500], Loss: 0.6122008562088013\n",
      "Epoch [500/500], Loss: 0.6122008562088013\n",
      "Epoch [100/500], Loss: 0.6286792755126953\n",
      "Epoch [200/500], Loss: 0.6286792755126953\n",
      "Epoch [300/500], Loss: 0.6286792755126953\n",
      "Epoch [400/500], Loss: 0.6286792755126953\n",
      "Epoch [500/500], Loss: 0.6286792755126953\n",
      "Epoch [100/500], Loss: 0.6451576948165894\n",
      "Epoch [200/500], Loss: 0.6451576948165894\n",
      "Epoch [300/500], Loss: 0.6451576948165894\n",
      "Epoch [400/500], Loss: 0.6451576948165894\n",
      "Epoch [500/500], Loss: 0.6451576948165894\n",
      "Epoch [100/500], Loss: 0.6616361141204834\n",
      "Epoch [200/500], Loss: 0.6616361141204834\n",
      "Epoch [300/500], Loss: 0.6616361141204834\n",
      "Epoch [400/500], Loss: 0.6616361141204834\n",
      "Epoch [500/500], Loss: 0.6616361141204834\n",
      "Epoch [100/500], Loss: 0.6781144738197327\n",
      "Epoch [200/500], Loss: 0.6781144738197327\n",
      "Epoch [300/500], Loss: 0.6781144738197327\n",
      "Epoch [400/500], Loss: 0.6781144738197327\n",
      "Epoch [500/500], Loss: 0.6781144738197327\n",
      "Epoch [100/500], Loss: 0.6945929527282715\n",
      "Epoch [200/500], Loss: 0.6945929527282715\n",
      "Epoch [300/500], Loss: 0.6945929527282715\n",
      "Epoch [400/500], Loss: 0.6945929527282715\n",
      "Epoch [500/500], Loss: 0.6945929527282715\n",
      "Epoch [100/500], Loss: 0.7110713124275208\n",
      "Epoch [200/500], Loss: 0.7110713124275208\n",
      "Epoch [300/500], Loss: 0.7110713124275208\n",
      "Epoch [400/500], Loss: 0.7110713124275208\n",
      "Epoch [500/500], Loss: 0.7110713124275208\n",
      "Epoch [100/500], Loss: 0.7275497317314148\n",
      "Epoch [200/500], Loss: 0.7275497317314148\n",
      "Epoch [300/500], Loss: 0.7275497317314148\n",
      "Epoch [400/500], Loss: 0.7275497317314148\n",
      "Epoch [500/500], Loss: 0.7275497317314148\n",
      "Epoch [100/500], Loss: 0.7440281510353088\n",
      "Epoch [200/500], Loss: 0.7440281510353088\n",
      "Epoch [300/500], Loss: 0.7440281510353088\n",
      "Epoch [400/500], Loss: 0.7440281510353088\n",
      "Epoch [500/500], Loss: 0.7440281510353088\n",
      "Epoch [100/500], Loss: 0.7605065703392029\n",
      "Epoch [200/500], Loss: 0.7605065703392029\n",
      "Epoch [300/500], Loss: 0.7605065703392029\n",
      "Epoch [400/500], Loss: 0.7605065703392029\n",
      "Epoch [500/500], Loss: 0.7605065703392029\n",
      "Epoch [100/500], Loss: 0.7769849896430969\n",
      "Epoch [200/500], Loss: 0.7769849896430969\n",
      "Epoch [300/500], Loss: 0.7769849896430969\n",
      "Epoch [400/500], Loss: 0.7769849896430969\n",
      "Epoch [500/500], Loss: 0.7769849896430969\n",
      "Epoch [100/500], Loss: 0.793463408946991\n",
      "Epoch [200/500], Loss: 0.793463408946991\n",
      "Epoch [300/500], Loss: 0.793463408946991\n",
      "Epoch [400/500], Loss: 0.793463408946991\n",
      "Epoch [500/500], Loss: 0.793463408946991\n",
      "Epoch [100/500], Loss: 0.8099417686462402\n",
      "Epoch [200/500], Loss: 0.8099417686462402\n",
      "Epoch [300/500], Loss: 0.8099417686462402\n",
      "Epoch [400/500], Loss: 0.8099417686462402\n",
      "Epoch [500/500], Loss: 0.8099417686462402\n",
      "Epoch [100/500], Loss: 0.8264201879501343\n",
      "Epoch [200/500], Loss: 0.8264201879501343\n",
      "Epoch [300/500], Loss: 0.8264201879501343\n",
      "Epoch [400/500], Loss: 0.8264201879501343\n",
      "Epoch [500/500], Loss: 0.8264201879501343\n",
      "Epoch [100/500], Loss: 0.8428986668586731\n",
      "Epoch [200/500], Loss: 0.8428986668586731\n",
      "Epoch [300/500], Loss: 0.8428986668586731\n",
      "Epoch [400/500], Loss: 0.8428986668586731\n",
      "Epoch [500/500], Loss: 0.8428986668586731\n",
      "Epoch [100/500], Loss: 0.8593770265579224\n",
      "Epoch [200/500], Loss: 0.8593770265579224\n",
      "Epoch [300/500], Loss: 0.8593770265579224\n",
      "Epoch [400/500], Loss: 0.8593770265579224\n",
      "Epoch [500/500], Loss: 0.8593770265579224\n",
      "Epoch [100/500], Loss: 0.8758554458618164\n",
      "Epoch [200/500], Loss: 0.8758554458618164\n",
      "Epoch [300/500], Loss: 0.8758554458618164\n",
      "Epoch [400/500], Loss: 0.8758554458618164\n",
      "Epoch [500/500], Loss: 0.8758554458618164\n",
      "Epoch [100/500], Loss: 0.8923338651657104\n",
      "Epoch [200/500], Loss: 0.8923338651657104\n",
      "Epoch [300/500], Loss: 0.8923338651657104\n",
      "Epoch [400/500], Loss: 0.8923338651657104\n",
      "Epoch [500/500], Loss: 0.8923338651657104\n",
      "Epoch [100/500], Loss: 0.9088122844696045\n",
      "Epoch [200/500], Loss: 0.9088122844696045\n",
      "Epoch [300/500], Loss: 0.9088122844696045\n",
      "Epoch [400/500], Loss: 0.9088122844696045\n",
      "Epoch [500/500], Loss: 0.9088122844696045\n",
      "Epoch [100/500], Loss: 0.9252907037734985\n",
      "Epoch [200/500], Loss: 0.9252907037734985\n",
      "Epoch [300/500], Loss: 0.9252907037734985\n",
      "Epoch [400/500], Loss: 0.9252907037734985\n",
      "Epoch [500/500], Loss: 0.9252907037734985\n",
      "Epoch [100/500], Loss: 0.9417691230773926\n",
      "Epoch [200/500], Loss: 0.9417691230773926\n",
      "Epoch [300/500], Loss: 0.9417691230773926\n",
      "Epoch [400/500], Loss: 0.9417691230773926\n",
      "Epoch [500/500], Loss: 0.9417691230773926\n",
      "Epoch [100/500], Loss: 0.9582475423812866\n",
      "Epoch [200/500], Loss: 0.9582475423812866\n",
      "Epoch [300/500], Loss: 0.9582475423812866\n",
      "Epoch [400/500], Loss: 0.9582475423812866\n",
      "Epoch [500/500], Loss: 0.9582475423812866\n",
      "Epoch [100/500], Loss: 0.9747259616851807\n",
      "Epoch [200/500], Loss: 0.9747259616851807\n",
      "Epoch [300/500], Loss: 0.9747259616851807\n",
      "Epoch [400/500], Loss: 0.9747259616851807\n",
      "Epoch [500/500], Loss: 0.9747259616851807\n",
      "Epoch [100/500], Loss: 0.9912043809890747\n",
      "Epoch [200/500], Loss: 0.9912043809890747\n",
      "Epoch [300/500], Loss: 0.9912043809890747\n",
      "Epoch [400/500], Loss: 0.9912043809890747\n",
      "Epoch [500/500], Loss: 0.9912043809890747\n",
      "Epoch [100/500], Loss: 1.0076828002929688\n",
      "Epoch [200/500], Loss: 1.0076828002929688\n",
      "Epoch [300/500], Loss: 1.0076828002929688\n",
      "Epoch [400/500], Loss: 1.0076828002929688\n",
      "Epoch [500/500], Loss: 1.0076828002929688\n",
      "Epoch [100/500], Loss: 1.0241612195968628\n",
      "Epoch [200/500], Loss: 1.0241612195968628\n",
      "Epoch [300/500], Loss: 1.0241612195968628\n",
      "Epoch [400/500], Loss: 1.0241612195968628\n",
      "Epoch [500/500], Loss: 1.0241612195968628\n",
      "Epoch [100/500], Loss: 1.0406396389007568\n",
      "Epoch [200/500], Loss: 1.0406396389007568\n",
      "Epoch [300/500], Loss: 1.0406396389007568\n",
      "Epoch [400/500], Loss: 1.0406396389007568\n",
      "Epoch [500/500], Loss: 1.0406396389007568\n",
      "Epoch [100/500], Loss: 1.0571180582046509\n",
      "Epoch [200/500], Loss: 1.0571180582046509\n",
      "Epoch [300/500], Loss: 1.0571180582046509\n",
      "Epoch [400/500], Loss: 1.0571180582046509\n",
      "Epoch [500/500], Loss: 1.0571180582046509\n",
      "Epoch [100/500], Loss: 1.073596477508545\n",
      "Epoch [200/500], Loss: 1.073596477508545\n",
      "Epoch [300/500], Loss: 1.073596477508545\n",
      "Epoch [400/500], Loss: 1.073596477508545\n",
      "Epoch [500/500], Loss: 1.073596477508545\n",
      "Epoch [100/500], Loss: 1.090074896812439\n",
      "Epoch [200/500], Loss: 1.090074896812439\n",
      "Epoch [300/500], Loss: 1.090074896812439\n",
      "Epoch [400/500], Loss: 1.090074896812439\n",
      "Epoch [500/500], Loss: 1.090074896812439\n",
      "Epoch [100/500], Loss: 1.106553316116333\n",
      "Epoch [200/500], Loss: 1.106553316116333\n",
      "Epoch [300/500], Loss: 1.106553316116333\n",
      "Epoch [400/500], Loss: 1.106553316116333\n",
      "Epoch [500/500], Loss: 1.106553316116333\n",
      "Epoch [100/500], Loss: 1.1230316162109375\n",
      "Epoch [200/500], Loss: 1.1230316162109375\n",
      "Epoch [300/500], Loss: 1.1230316162109375\n",
      "Epoch [400/500], Loss: 1.1230316162109375\n",
      "Epoch [500/500], Loss: 1.1230316162109375\n",
      "Epoch [100/500], Loss: 1.139510154724121\n",
      "Epoch [200/500], Loss: 1.139510154724121\n",
      "Epoch [300/500], Loss: 1.139510154724121\n",
      "Epoch [400/500], Loss: 1.139510154724121\n",
      "Epoch [500/500], Loss: 1.139510154724121\n",
      "Epoch [100/500], Loss: 1.1559884548187256\n",
      "Epoch [200/500], Loss: 1.1559884548187256\n",
      "Epoch [300/500], Loss: 1.1559884548187256\n",
      "Epoch [400/500], Loss: 1.1559884548187256\n",
      "Epoch [500/500], Loss: 1.1559884548187256\n",
      "Epoch [100/500], Loss: 1.1724669933319092\n",
      "Epoch [200/500], Loss: 1.1724669933319092\n",
      "Epoch [300/500], Loss: 1.1724669933319092\n",
      "Epoch [400/500], Loss: 1.1724669933319092\n",
      "Epoch [500/500], Loss: 1.1724669933319092\n",
      "Epoch [100/500], Loss: 1.1889452934265137\n",
      "Epoch [200/500], Loss: 1.1889452934265137\n",
      "Epoch [300/500], Loss: 1.1889452934265137\n",
      "Epoch [400/500], Loss: 1.1889452934265137\n",
      "Epoch [500/500], Loss: 1.1889452934265137\n",
      "Epoch [100/500], Loss: 1.2054238319396973\n",
      "Epoch [200/500], Loss: 1.2054238319396973\n",
      "Epoch [300/500], Loss: 1.2054238319396973\n",
      "Epoch [400/500], Loss: 1.2054238319396973\n",
      "Epoch [500/500], Loss: 1.2054238319396973\n",
      "Epoch [100/500], Loss: 1.2219021320343018\n",
      "Epoch [200/500], Loss: 1.2219021320343018\n",
      "Epoch [300/500], Loss: 1.2219021320343018\n",
      "Epoch [400/500], Loss: 1.2219021320343018\n",
      "Epoch [500/500], Loss: 1.2219021320343018\n",
      "Epoch [100/500], Loss: 1.2383805513381958\n",
      "Epoch [200/500], Loss: 1.2383805513381958\n",
      "Epoch [300/500], Loss: 1.2383805513381958\n",
      "Epoch [400/500], Loss: 1.2383805513381958\n",
      "Epoch [500/500], Loss: 1.2383805513381958\n",
      "Epoch [100/500], Loss: 1.2548589706420898\n",
      "Epoch [200/500], Loss: 1.2548589706420898\n",
      "Epoch [300/500], Loss: 1.2548589706420898\n",
      "Epoch [400/500], Loss: 1.2548589706420898\n",
      "Epoch [500/500], Loss: 1.2548589706420898\n",
      "Epoch [100/500], Loss: 1.2713373899459839\n",
      "Epoch [200/500], Loss: 1.2713373899459839\n",
      "Epoch [300/500], Loss: 1.2713373899459839\n",
      "Epoch [400/500], Loss: 1.2713373899459839\n",
      "Epoch [500/500], Loss: 1.2713373899459839\n",
      "Epoch [100/500], Loss: 1.287815809249878\n",
      "Epoch [200/500], Loss: 1.287815809249878\n",
      "Epoch [300/500], Loss: 1.287815809249878\n",
      "Epoch [400/500], Loss: 1.287815809249878\n",
      "Epoch [500/500], Loss: 1.287815809249878\n",
      "Epoch [100/500], Loss: 1.3042943477630615\n",
      "Epoch [200/500], Loss: 1.3042943477630615\n",
      "Epoch [300/500], Loss: 1.3042943477630615\n",
      "Epoch [400/500], Loss: 1.3042943477630615\n",
      "Epoch [500/500], Loss: 1.3042943477630615\n",
      "Epoch [100/500], Loss: 1.320772647857666\n",
      "Epoch [200/500], Loss: 1.320772647857666\n",
      "Epoch [300/500], Loss: 1.320772647857666\n",
      "Epoch [400/500], Loss: 1.320772647857666\n",
      "Epoch [500/500], Loss: 1.320772647857666\n",
      "Epoch [100/500], Loss: 1.33725106716156\n",
      "Epoch [200/500], Loss: 1.33725106716156\n",
      "Epoch [300/500], Loss: 1.33725106716156\n",
      "Epoch [400/500], Loss: 1.33725106716156\n",
      "Epoch [500/500], Loss: 1.33725106716156\n",
      "Epoch [100/500], Loss: 1.353729486465454\n",
      "Epoch [200/500], Loss: 1.353729486465454\n",
      "Epoch [300/500], Loss: 1.353729486465454\n",
      "Epoch [400/500], Loss: 1.353729486465454\n",
      "Epoch [500/500], Loss: 1.353729486465454\n",
      "Epoch [100/500], Loss: 1.3702079057693481\n",
      "Epoch [200/500], Loss: 1.3702079057693481\n",
      "Epoch [300/500], Loss: 1.3702079057693481\n",
      "Epoch [400/500], Loss: 1.3702079057693481\n",
      "Epoch [500/500], Loss: 1.3702079057693481\n",
      "Epoch [100/500], Loss: 1.3866863250732422\n",
      "Epoch [200/500], Loss: 1.3866863250732422\n",
      "Epoch [300/500], Loss: 1.3866863250732422\n",
      "Epoch [400/500], Loss: 1.3866863250732422\n",
      "Epoch [500/500], Loss: 1.3866863250732422\n",
      "Epoch [100/500], Loss: 1.4031646251678467\n",
      "Epoch [200/500], Loss: 1.4031646251678467\n",
      "Epoch [300/500], Loss: 1.4031646251678467\n",
      "Epoch [400/500], Loss: 1.4031646251678467\n",
      "Epoch [500/500], Loss: 1.4031646251678467\n",
      "Epoch [100/500], Loss: 1.4196431636810303\n",
      "Epoch [200/500], Loss: 1.4196431636810303\n",
      "Epoch [300/500], Loss: 1.4196431636810303\n",
      "Epoch [400/500], Loss: 1.4196431636810303\n",
      "Epoch [500/500], Loss: 1.4196431636810303\n",
      "Epoch [100/500], Loss: 1.4361215829849243\n",
      "Epoch [200/500], Loss: 1.4361215829849243\n",
      "Epoch [300/500], Loss: 1.4361215829849243\n",
      "Epoch [400/500], Loss: 1.4361215829849243\n",
      "Epoch [500/500], Loss: 1.4361215829849243\n",
      "Epoch [100/500], Loss: 1.4526000022888184\n",
      "Epoch [200/500], Loss: 1.4526000022888184\n",
      "Epoch [300/500], Loss: 1.4526000022888184\n",
      "Epoch [400/500], Loss: 1.4526000022888184\n",
      "Epoch [500/500], Loss: 1.4526000022888184\n",
      "Epoch [100/500], Loss: 1.4690784215927124\n",
      "Epoch [200/500], Loss: 1.4690784215927124\n",
      "Epoch [300/500], Loss: 1.4690784215927124\n",
      "Epoch [400/500], Loss: 1.4690784215927124\n",
      "Epoch [500/500], Loss: 1.4690784215927124\n",
      "Epoch [100/500], Loss: 1.4855568408966064\n",
      "Epoch [200/500], Loss: 1.4855568408966064\n",
      "Epoch [300/500], Loss: 1.4855568408966064\n",
      "Epoch [400/500], Loss: 1.4855568408966064\n",
      "Epoch [500/500], Loss: 1.4855568408966064\n",
      "Epoch [100/500], Loss: 1.502035140991211\n",
      "Epoch [200/500], Loss: 1.502035140991211\n",
      "Epoch [300/500], Loss: 1.502035140991211\n",
      "Epoch [400/500], Loss: 1.502035140991211\n",
      "Epoch [500/500], Loss: 1.502035140991211\n",
      "Epoch [100/500], Loss: 1.5185136795043945\n",
      "Epoch [200/500], Loss: 1.5185136795043945\n",
      "Epoch [300/500], Loss: 1.5185136795043945\n",
      "Epoch [400/500], Loss: 1.5185136795043945\n",
      "Epoch [500/500], Loss: 1.5185136795043945\n",
      "Epoch [100/500], Loss: 1.5349920988082886\n",
      "Epoch [200/500], Loss: 1.5349920988082886\n",
      "Epoch [300/500], Loss: 1.5349920988082886\n",
      "Epoch [400/500], Loss: 1.5349920988082886\n",
      "Epoch [500/500], Loss: 1.5349920988082886\n",
      "Epoch [100/500], Loss: 1.5514705181121826\n",
      "Epoch [200/500], Loss: 1.5514705181121826\n",
      "Epoch [300/500], Loss: 1.5514705181121826\n",
      "Epoch [400/500], Loss: 1.5514705181121826\n",
      "Epoch [500/500], Loss: 1.5514705181121826\n",
      "Epoch [100/500], Loss: 1.5679489374160767\n",
      "Epoch [200/500], Loss: 1.5679489374160767\n",
      "Epoch [300/500], Loss: 1.5679489374160767\n",
      "Epoch [400/500], Loss: 1.5679489374160767\n",
      "Epoch [500/500], Loss: 1.5679489374160767\n",
      "Epoch [100/500], Loss: 1.5844272375106812\n",
      "Epoch [200/500], Loss: 1.5844272375106812\n",
      "Epoch [300/500], Loss: 1.5844272375106812\n",
      "Epoch [400/500], Loss: 1.5844272375106812\n",
      "Epoch [500/500], Loss: 1.5844272375106812\n",
      "Epoch [100/500], Loss: 1.6009056568145752\n",
      "Epoch [200/500], Loss: 1.6009056568145752\n",
      "Epoch [300/500], Loss: 1.6009056568145752\n",
      "Epoch [400/500], Loss: 1.6009056568145752\n",
      "Epoch [500/500], Loss: 1.6009056568145752\n",
      "Epoch [100/500], Loss: 1.6173840761184692\n",
      "Epoch [200/500], Loss: 1.6173840761184692\n",
      "Epoch [300/500], Loss: 1.6173840761184692\n",
      "Epoch [400/500], Loss: 1.6173840761184692\n",
      "Epoch [500/500], Loss: 1.6173840761184692\n",
      "Epoch [100/500], Loss: 1.6338626146316528\n",
      "Epoch [200/500], Loss: 1.6338626146316528\n",
      "Epoch [300/500], Loss: 1.6338626146316528\n",
      "Epoch [400/500], Loss: 1.6338626146316528\n",
      "Epoch [500/500], Loss: 1.6338626146316528\n",
      "Epoch [100/500], Loss: 1.6503410339355469\n",
      "Epoch [200/500], Loss: 1.6503410339355469\n",
      "Epoch [300/500], Loss: 1.6503410339355469\n",
      "Epoch [400/500], Loss: 1.6503410339355469\n",
      "Epoch [500/500], Loss: 1.6503410339355469\n",
      "Epoch [100/500], Loss: 1.6668193340301514\n",
      "Epoch [200/500], Loss: 1.6668193340301514\n",
      "Epoch [300/500], Loss: 1.6668193340301514\n",
      "Epoch [400/500], Loss: 1.6668193340301514\n",
      "Epoch [500/500], Loss: 1.6668193340301514\n",
      "Epoch [100/500], Loss: 1.6832977533340454\n",
      "Epoch [200/500], Loss: 1.6832977533340454\n",
      "Epoch [300/500], Loss: 1.6832977533340454\n",
      "Epoch [400/500], Loss: 1.6832977533340454\n",
      "Epoch [500/500], Loss: 1.6832977533340454\n",
      "Epoch [100/500], Loss: 1.6997761726379395\n",
      "Epoch [200/500], Loss: 1.6997761726379395\n",
      "Epoch [300/500], Loss: 1.6997761726379395\n",
      "Epoch [400/500], Loss: 1.6997761726379395\n",
      "Epoch [500/500], Loss: 1.6997761726379395\n",
      "Epoch [100/500], Loss: 1.7162545919418335\n",
      "Epoch [200/500], Loss: 1.7162545919418335\n",
      "Epoch [300/500], Loss: 1.7162545919418335\n",
      "Epoch [400/500], Loss: 1.7162545919418335\n",
      "Epoch [500/500], Loss: 1.7162545919418335\n",
      "Epoch [100/500], Loss: 1.732733130455017\n",
      "Epoch [200/500], Loss: 1.732733130455017\n",
      "Epoch [300/500], Loss: 1.732733130455017\n",
      "Epoch [400/500], Loss: 1.732733130455017\n",
      "Epoch [500/500], Loss: 1.732733130455017\n",
      "Epoch [100/500], Loss: 1.7492114305496216\n",
      "Epoch [200/500], Loss: 1.7492114305496216\n",
      "Epoch [300/500], Loss: 1.7492114305496216\n",
      "Epoch [400/500], Loss: 1.7492114305496216\n",
      "Epoch [500/500], Loss: 1.7492114305496216\n",
      "Epoch [100/500], Loss: 1.7656898498535156\n",
      "Epoch [200/500], Loss: 1.7656898498535156\n",
      "Epoch [300/500], Loss: 1.7656898498535156\n",
      "Epoch [400/500], Loss: 1.7656898498535156\n",
      "Epoch [500/500], Loss: 1.7656898498535156\n",
      "Epoch [100/500], Loss: 1.7821682691574097\n",
      "Epoch [200/500], Loss: 1.7821682691574097\n",
      "Epoch [300/500], Loss: 1.7821682691574097\n",
      "Epoch [400/500], Loss: 1.7821682691574097\n",
      "Epoch [500/500], Loss: 1.7821682691574097\n",
      "Epoch [100/500], Loss: 1.7986466884613037\n",
      "Epoch [200/500], Loss: 1.7986466884613037\n",
      "Epoch [300/500], Loss: 1.7986466884613037\n",
      "Epoch [400/500], Loss: 1.7986466884613037\n",
      "Epoch [500/500], Loss: 1.7986466884613037\n",
      "Epoch [100/500], Loss: 1.8151251077651978\n",
      "Epoch [200/500], Loss: 1.8151251077651978\n",
      "Epoch [300/500], Loss: 1.8151251077651978\n",
      "Epoch [400/500], Loss: 1.8151251077651978\n",
      "Epoch [500/500], Loss: 1.8151251077651978\n",
      "Epoch [100/500], Loss: 1.8316034078598022\n",
      "Epoch [200/500], Loss: 1.8316034078598022\n",
      "Epoch [300/500], Loss: 1.8316034078598022\n",
      "Epoch [400/500], Loss: 1.8316034078598022\n",
      "Epoch [500/500], Loss: 1.8316034078598022\n",
      "Epoch [100/500], Loss: 1.8480819463729858\n",
      "Epoch [200/500], Loss: 1.8480819463729858\n",
      "Epoch [300/500], Loss: 1.8480819463729858\n",
      "Epoch [400/500], Loss: 1.8480819463729858\n",
      "Epoch [500/500], Loss: 1.8480819463729858\n",
      "Epoch [100/500], Loss: 1.8645603656768799\n",
      "Epoch [200/500], Loss: 1.8645603656768799\n",
      "Epoch [300/500], Loss: 1.8645603656768799\n",
      "Epoch [400/500], Loss: 1.8645603656768799\n",
      "Epoch [500/500], Loss: 1.8645603656768799\n",
      "Epoch [100/500], Loss: 1.881038784980774\n",
      "Epoch [200/500], Loss: 1.881038784980774\n",
      "Epoch [300/500], Loss: 1.881038784980774\n",
      "Epoch [400/500], Loss: 1.881038784980774\n",
      "Epoch [500/500], Loss: 1.881038784980774\n",
      "Epoch [100/500], Loss: 1.897517204284668\n",
      "Epoch [200/500], Loss: 1.897517204284668\n",
      "Epoch [300/500], Loss: 1.897517204284668\n",
      "Epoch [400/500], Loss: 1.897517204284668\n",
      "Epoch [500/500], Loss: 1.897517204284668\n",
      "Epoch [100/500], Loss: 1.9139955043792725\n",
      "Epoch [200/500], Loss: 1.9139955043792725\n",
      "Epoch [300/500], Loss: 1.9139955043792725\n",
      "Epoch [400/500], Loss: 1.9139955043792725\n",
      "Epoch [500/500], Loss: 1.9139955043792725\n",
      "Epoch [100/500], Loss: 1.9304739236831665\n",
      "Epoch [200/500], Loss: 1.9304739236831665\n",
      "Epoch [300/500], Loss: 1.9304739236831665\n",
      "Epoch [400/500], Loss: 1.9304739236831665\n",
      "Epoch [500/500], Loss: 1.9304739236831665\n",
      "Epoch [100/500], Loss: 1.94695246219635\n",
      "Epoch [200/500], Loss: 1.94695246219635\n",
      "Epoch [300/500], Loss: 1.94695246219635\n",
      "Epoch [400/500], Loss: 1.94695246219635\n",
      "Epoch [500/500], Loss: 1.94695246219635\n",
      "Epoch [100/500], Loss: 1.9634308815002441\n",
      "Epoch [200/500], Loss: 1.9634308815002441\n",
      "Epoch [300/500], Loss: 1.9634308815002441\n",
      "Epoch [400/500], Loss: 1.9634308815002441\n",
      "Epoch [500/500], Loss: 1.9634308815002441\n",
      "Epoch [100/500], Loss: 1.9799093008041382\n",
      "Epoch [200/500], Loss: 1.9799093008041382\n",
      "Epoch [300/500], Loss: 1.9799093008041382\n",
      "Epoch [400/500], Loss: 1.9799093008041382\n",
      "Epoch [500/500], Loss: 1.9799093008041382\n",
      "Epoch [100/500], Loss: 1.9963876008987427\n",
      "Epoch [200/500], Loss: 1.9963876008987427\n",
      "Epoch [300/500], Loss: 1.9963876008987427\n",
      "Epoch [400/500], Loss: 1.9963876008987427\n",
      "Epoch [500/500], Loss: 1.9963876008987427\n",
      "Epoch [100/500], Loss: 2.0128660202026367\n",
      "Epoch [200/500], Loss: 2.0128660202026367\n",
      "Epoch [300/500], Loss: 2.0128660202026367\n",
      "Epoch [400/500], Loss: 2.0128660202026367\n",
      "Epoch [500/500], Loss: 2.0128660202026367\n",
      "Epoch [100/500], Loss: 2.0293445587158203\n",
      "Epoch [200/500], Loss: 2.0293445587158203\n",
      "Epoch [300/500], Loss: 2.0293445587158203\n",
      "Epoch [400/500], Loss: 2.0293445587158203\n",
      "Epoch [500/500], Loss: 2.0293445587158203\n",
      "Epoch [100/500], Loss: 2.045823097229004\n",
      "Epoch [200/500], Loss: 2.045823097229004\n",
      "Epoch [300/500], Loss: 2.045823097229004\n",
      "Epoch [400/500], Loss: 2.045823097229004\n",
      "Epoch [500/500], Loss: 2.045823097229004\n",
      "Epoch [100/500], Loss: 2.0623013973236084\n",
      "Epoch [200/500], Loss: 2.0623013973236084\n",
      "Epoch [300/500], Loss: 2.0623013973236084\n",
      "Epoch [400/500], Loss: 2.0623013973236084\n",
      "Epoch [500/500], Loss: 2.0623013973236084\n",
      "Epoch [100/500], Loss: 2.078779697418213\n",
      "Epoch [200/500], Loss: 2.078779697418213\n",
      "Epoch [300/500], Loss: 2.078779697418213\n",
      "Epoch [400/500], Loss: 2.078779697418213\n",
      "Epoch [500/500], Loss: 2.078779697418213\n",
      "Epoch [100/500], Loss: 2.0952582359313965\n",
      "Epoch [200/500], Loss: 2.0952582359313965\n",
      "Epoch [300/500], Loss: 2.0952582359313965\n",
      "Epoch [400/500], Loss: 2.0952582359313965\n",
      "Epoch [500/500], Loss: 2.0952582359313965\n",
      "Epoch [100/500], Loss: 2.111736536026001\n",
      "Epoch [200/500], Loss: 2.111736536026001\n",
      "Epoch [300/500], Loss: 2.111736536026001\n",
      "Epoch [400/500], Loss: 2.111736536026001\n",
      "Epoch [500/500], Loss: 2.111736536026001\n",
      "Epoch [100/500], Loss: 2.1282148361206055\n",
      "Epoch [200/500], Loss: 2.1282148361206055\n",
      "Epoch [300/500], Loss: 2.1282148361206055\n",
      "Epoch [400/500], Loss: 2.1282148361206055\n",
      "Epoch [500/500], Loss: 2.1282148361206055\n",
      "Epoch [100/500], Loss: 2.144693374633789\n",
      "Epoch [200/500], Loss: 2.144693374633789\n",
      "Epoch [300/500], Loss: 2.144693374633789\n",
      "Epoch [400/500], Loss: 2.144693374633789\n",
      "Epoch [500/500], Loss: 2.144693374633789\n",
      "Epoch [100/500], Loss: 2.1611719131469727\n",
      "Epoch [200/500], Loss: 2.1611719131469727\n",
      "Epoch [300/500], Loss: 2.1611719131469727\n",
      "Epoch [400/500], Loss: 2.1611719131469727\n",
      "Epoch [500/500], Loss: 2.1611719131469727\n",
      "Epoch [100/500], Loss: 2.177650213241577\n",
      "Epoch [200/500], Loss: 2.177650213241577\n",
      "Epoch [300/500], Loss: 2.177650213241577\n",
      "Epoch [400/500], Loss: 2.177650213241577\n",
      "Epoch [500/500], Loss: 2.177650213241577\n",
      "Epoch [100/500], Loss: 0.20514437556266785\n",
      "Epoch [200/500], Loss: 0.17974866926670074\n",
      "Epoch [300/500], Loss: 0.1762145310640335\n",
      "Epoch [400/500], Loss: 0.17560981214046478\n",
      "Epoch [500/500], Loss: 0.17549830675125122\n",
      "Epoch [100/500], Loss: 0.2580765187740326\n",
      "Epoch [200/500], Loss: 0.25766947865486145\n",
      "Epoch [300/500], Loss: 0.2562635540962219\n",
      "Epoch [400/500], Loss: 0.25564706325531006\n",
      "Epoch [500/500], Loss: 0.2554461359977722\n",
      "Epoch [100/500], Loss: 0.3153707981109619\n",
      "Epoch [200/500], Loss: 0.3155204653739929\n",
      "Epoch [300/500], Loss: 0.3141825497150421\n",
      "Epoch [400/500], Loss: 0.3154972493648529\n",
      "Epoch [500/500], Loss: 0.31495338678359985\n",
      "Epoch [100/500], Loss: 0.3616628646850586\n",
      "Epoch [200/500], Loss: 0.3570738434791565\n",
      "Epoch [300/500], Loss: 0.3599683940410614\n",
      "Epoch [400/500], Loss: 0.35051363706588745\n",
      "Epoch [500/500], Loss: 0.3604171574115753\n",
      "Epoch [100/500], Loss: 0.3949281573295593\n",
      "Epoch [200/500], Loss: 0.3881545066833496\n",
      "Epoch [300/500], Loss: 0.3846840262413025\n",
      "Epoch [400/500], Loss: 0.38788068294525146\n",
      "Epoch [500/500], Loss: 0.3841240704059601\n",
      "Epoch [100/500], Loss: 0.4164850115776062\n",
      "Epoch [200/500], Loss: 0.40286552906036377\n",
      "Epoch [300/500], Loss: 0.40756452083587646\n",
      "Epoch [400/500], Loss: 0.401294082403183\n",
      "Epoch [500/500], Loss: 0.4177873134613037\n",
      "Epoch [100/500], Loss: 0.43969500064849854\n",
      "Epoch [200/500], Loss: 0.43431732058525085\n",
      "Epoch [300/500], Loss: 0.4319050908088684\n",
      "Epoch [400/500], Loss: 0.4369717836380005\n",
      "Epoch [500/500], Loss: 0.4233160614967346\n",
      "Epoch [100/500], Loss: 0.44911354780197144\n",
      "Epoch [200/500], Loss: 0.45463794469833374\n",
      "Epoch [300/500], Loss: 0.45555582642555237\n",
      "Epoch [400/500], Loss: 0.4660782814025879\n",
      "Epoch [500/500], Loss: 0.44482526183128357\n",
      "Epoch [100/500], Loss: 0.48243778944015503\n",
      "Epoch [200/500], Loss: 0.48935550451278687\n",
      "Epoch [300/500], Loss: 0.4765320420265198\n",
      "Epoch [400/500], Loss: 0.47806382179260254\n",
      "Epoch [500/500], Loss: 0.49270278215408325\n",
      "Epoch [100/500], Loss: 0.4935353994369507\n",
      "Epoch [200/500], Loss: 0.5035101771354675\n",
      "Epoch [300/500], Loss: 0.524976372718811\n",
      "Epoch [400/500], Loss: 0.4922768771648407\n",
      "Epoch [500/500], Loss: 0.4919167757034302\n",
      "Epoch [100/500], Loss: 0.5340743064880371\n",
      "Epoch [200/500], Loss: 0.524137556552887\n",
      "Epoch [300/500], Loss: 0.5217306017875671\n",
      "Epoch [400/500], Loss: 0.5211060047149658\n",
      "Epoch [500/500], Loss: 0.5190824270248413\n",
      "Epoch [100/500], Loss: 0.5293020606040955\n",
      "Epoch [200/500], Loss: 0.5278691649436951\n",
      "Epoch [300/500], Loss: 0.5482668280601501\n",
      "Epoch [400/500], Loss: 0.5410687923431396\n",
      "Epoch [500/500], Loss: 0.5598663687705994\n",
      "Epoch [100/500], Loss: 0.5411728620529175\n",
      "Epoch [200/500], Loss: 0.5487329959869385\n",
      "Epoch [300/500], Loss: 0.5651712417602539\n",
      "Epoch [400/500], Loss: 0.5421396493911743\n",
      "Epoch [500/500], Loss: 0.5503835678100586\n",
      "Epoch [100/500], Loss: 0.5753082633018494\n",
      "Epoch [200/500], Loss: 0.547394871711731\n",
      "Epoch [300/500], Loss: 0.5615891218185425\n",
      "Epoch [400/500], Loss: 0.565209150314331\n",
      "Epoch [500/500], Loss: 0.5607694983482361\n",
      "Epoch [100/500], Loss: 0.5821447372436523\n",
      "Epoch [200/500], Loss: 0.5723590850830078\n",
      "Epoch [300/500], Loss: 0.6109539270401001\n",
      "Epoch [400/500], Loss: 0.5955554246902466\n",
      "Epoch [500/500], Loss: 0.5870289206504822\n",
      "Epoch [100/500], Loss: 0.618148684501648\n",
      "Epoch [200/500], Loss: 0.5650444030761719\n",
      "Epoch [300/500], Loss: 0.6390182375907898\n",
      "Epoch [400/500], Loss: 0.597866415977478\n",
      "Epoch [500/500], Loss: 0.5731618404388428\n",
      "Epoch [100/500], Loss: 0.6128119230270386\n",
      "Epoch [200/500], Loss: 0.6017134785652161\n",
      "Epoch [300/500], Loss: 0.6214492321014404\n",
      "Epoch [400/500], Loss: 0.6088577508926392\n",
      "Epoch [500/500], Loss: 0.6055213212966919\n",
      "Epoch [100/500], Loss: 0.603792667388916\n",
      "Epoch [200/500], Loss: 0.662848949432373\n",
      "Epoch [300/500], Loss: 0.6279587745666504\n",
      "Epoch [400/500], Loss: 0.6183808445930481\n",
      "Epoch [500/500], Loss: 0.6118011474609375\n",
      "Epoch [100/500], Loss: 0.6051856279373169\n",
      "Epoch [200/500], Loss: 0.6244142055511475\n",
      "Epoch [300/500], Loss: 0.64052814245224\n",
      "Epoch [400/500], Loss: 0.6139914393424988\n",
      "Epoch [500/500], Loss: 0.5978101491928101\n",
      "Epoch [100/500], Loss: 0.6226193904876709\n",
      "Epoch [200/500], Loss: 0.6045459508895874\n",
      "Epoch [300/500], Loss: 0.6785154342651367\n",
      "Epoch [400/500], Loss: 0.6586824655532837\n",
      "Epoch [500/500], Loss: 0.6324461698532104\n",
      "Epoch [100/500], Loss: 0.6559215784072876\n",
      "Epoch [200/500], Loss: 0.6410671472549438\n",
      "Epoch [300/500], Loss: 0.6557443141937256\n",
      "Epoch [400/500], Loss: 0.6607366800308228\n",
      "Epoch [500/500], Loss: 0.6437939405441284\n",
      "Epoch [100/500], Loss: 0.6469088196754456\n",
      "Epoch [200/500], Loss: 0.6610147953033447\n",
      "Epoch [300/500], Loss: 0.6616948246955872\n",
      "Epoch [400/500], Loss: 0.6697620153427124\n",
      "Epoch [500/500], Loss: 0.6095290184020996\n",
      "Epoch [100/500], Loss: 0.6887879371643066\n",
      "Epoch [200/500], Loss: 0.6295480132102966\n",
      "Epoch [300/500], Loss: 0.6710000038146973\n",
      "Epoch [400/500], Loss: 0.7246780395507812\n",
      "Epoch [500/500], Loss: 0.7247401475906372\n",
      "Epoch [100/500], Loss: 0.660327136516571\n",
      "Epoch [200/500], Loss: 0.6626863479614258\n",
      "Epoch [300/500], Loss: 0.6753432750701904\n",
      "Epoch [400/500], Loss: 0.6772996783256531\n",
      "Epoch [500/500], Loss: 0.7306922078132629\n",
      "Epoch [100/500], Loss: 0.6888093948364258\n",
      "Epoch [200/500], Loss: 0.6920915842056274\n",
      "Epoch [300/500], Loss: 0.7262725830078125\n",
      "Epoch [400/500], Loss: 0.6929118037223816\n",
      "Epoch [500/500], Loss: 0.6945400238037109\n",
      "Epoch [100/500], Loss: 0.7774011492729187\n",
      "Epoch [200/500], Loss: 0.7015404105186462\n",
      "Epoch [300/500], Loss: 0.6721228361129761\n",
      "Epoch [400/500], Loss: 0.6868577003479004\n",
      "Epoch [500/500], Loss: 0.70064377784729\n",
      "Epoch [100/500], Loss: 0.7243678569793701\n",
      "Epoch [200/500], Loss: 0.7175581455230713\n",
      "Epoch [300/500], Loss: 0.7726466655731201\n",
      "Epoch [400/500], Loss: 0.7582578659057617\n",
      "Epoch [500/500], Loss: 0.674888551235199\n",
      "Epoch [100/500], Loss: 0.7551026940345764\n",
      "Epoch [200/500], Loss: 0.7214242219924927\n",
      "Epoch [300/500], Loss: 0.714756965637207\n",
      "Epoch [400/500], Loss: 0.705220103263855\n",
      "Epoch [500/500], Loss: 0.7865276336669922\n",
      "Epoch [100/500], Loss: 0.7499171495437622\n",
      "Epoch [200/500], Loss: 0.7556993961334229\n",
      "Epoch [300/500], Loss: 0.7747023105621338\n",
      "Epoch [400/500], Loss: 0.7533555626869202\n",
      "Epoch [500/500], Loss: 0.7221049070358276\n",
      "Epoch [100/500], Loss: 0.7594195008277893\n",
      "Epoch [200/500], Loss: 0.7820919156074524\n",
      "Epoch [300/500], Loss: 0.7807995676994324\n",
      "Epoch [400/500], Loss: 0.7961224317550659\n",
      "Epoch [500/500], Loss: 0.703575074672699\n",
      "Epoch [100/500], Loss: 0.8285303115844727\n",
      "Epoch [200/500], Loss: 0.8095120191574097\n",
      "Epoch [300/500], Loss: 0.7080864310264587\n",
      "Epoch [400/500], Loss: 0.8252087831497192\n",
      "Epoch [500/500], Loss: 0.7921338081359863\n",
      "Epoch [100/500], Loss: 0.8262112140655518\n",
      "Epoch [200/500], Loss: 0.8533834218978882\n",
      "Epoch [300/500], Loss: 0.8483479022979736\n",
      "Epoch [400/500], Loss: 0.8443711996078491\n",
      "Epoch [500/500], Loss: 0.7503323554992676\n",
      "Epoch [100/500], Loss: 0.7625942230224609\n",
      "Epoch [200/500], Loss: 0.7819900512695312\n",
      "Epoch [300/500], Loss: 0.8414878845214844\n",
      "Epoch [400/500], Loss: 0.7444888949394226\n",
      "Epoch [500/500], Loss: 0.8556322455406189\n",
      "Epoch [100/500], Loss: 0.8457044363021851\n",
      "Epoch [200/500], Loss: 0.7833291292190552\n",
      "Epoch [300/500], Loss: 0.880294919013977\n",
      "Epoch [400/500], Loss: 0.8409611582756042\n",
      "Epoch [500/500], Loss: 0.7852096557617188\n",
      "Epoch [100/500], Loss: 0.8960505127906799\n",
      "Epoch [200/500], Loss: 0.8889389038085938\n",
      "Epoch [300/500], Loss: 0.8534008264541626\n",
      "Epoch [400/500], Loss: 0.8674753904342651\n",
      "Epoch [500/500], Loss: 0.7746037840843201\n",
      "Epoch [100/500], Loss: 0.8490266799926758\n",
      "Epoch [200/500], Loss: 0.8926932215690613\n",
      "Epoch [300/500], Loss: 0.8348022699356079\n",
      "Epoch [400/500], Loss: 0.7967061996459961\n",
      "Epoch [500/500], Loss: 0.8716377019882202\n",
      "Epoch [100/500], Loss: 0.8588186502456665\n",
      "Epoch [200/500], Loss: 0.8569676876068115\n",
      "Epoch [300/500], Loss: 0.8318915963172913\n",
      "Epoch [400/500], Loss: 0.8416739702224731\n",
      "Epoch [500/500], Loss: 0.8763264417648315\n",
      "Epoch [100/500], Loss: 0.8287303447723389\n",
      "Epoch [200/500], Loss: 0.9016851186752319\n",
      "Epoch [300/500], Loss: 0.8943144083023071\n",
      "Epoch [400/500], Loss: 0.9669662117958069\n",
      "Epoch [500/500], Loss: 0.891638994216919\n",
      "Epoch [100/500], Loss: 0.8942296504974365\n",
      "Epoch [200/500], Loss: 0.8807404041290283\n",
      "Epoch [300/500], Loss: 0.8944162130355835\n",
      "Epoch [400/500], Loss: 0.872017502784729\n",
      "Epoch [500/500], Loss: 0.8477507829666138\n",
      "Epoch [100/500], Loss: 0.9773937463760376\n",
      "Epoch [200/500], Loss: 0.9516170620918274\n",
      "Epoch [300/500], Loss: 0.8673439025878906\n",
      "Epoch [400/500], Loss: 0.8722316026687622\n",
      "Epoch [500/500], Loss: 0.8758670091629028\n",
      "Epoch [100/500], Loss: 0.9213699102401733\n",
      "Epoch [200/500], Loss: 0.9479186534881592\n",
      "Epoch [300/500], Loss: 0.9289076328277588\n",
      "Epoch [400/500], Loss: 0.9175171256065369\n",
      "Epoch [500/500], Loss: 0.9849677681922913\n",
      "Epoch [100/500], Loss: 0.9788908958435059\n",
      "Epoch [200/500], Loss: 0.9378169775009155\n",
      "Epoch [300/500], Loss: 0.9581266045570374\n",
      "Epoch [400/500], Loss: 0.8964571952819824\n",
      "Epoch [500/500], Loss: 1.0356950759887695\n",
      "Epoch [100/500], Loss: 0.9973874092102051\n",
      "Epoch [200/500], Loss: 0.9299929738044739\n",
      "Epoch [300/500], Loss: 0.9956540465354919\n",
      "Epoch [400/500], Loss: 0.9191864132881165\n",
      "Epoch [500/500], Loss: 0.8512367010116577\n",
      "Epoch [100/500], Loss: 1.0235670804977417\n",
      "Epoch [200/500], Loss: 0.9429129362106323\n",
      "Epoch [300/500], Loss: 0.9873917698860168\n",
      "Epoch [400/500], Loss: 0.9862134456634521\n",
      "Epoch [500/500], Loss: 1.0231796503067017\n",
      "Epoch [100/500], Loss: 1.010246753692627\n",
      "Epoch [200/500], Loss: 0.9701195955276489\n",
      "Epoch [300/500], Loss: 0.9777981042861938\n",
      "Epoch [400/500], Loss: 1.0304896831512451\n",
      "Epoch [500/500], Loss: 1.082690715789795\n",
      "Epoch [100/500], Loss: 0.9105978012084961\n",
      "Epoch [200/500], Loss: 0.9001766443252563\n",
      "Epoch [300/500], Loss: 0.9982079267501831\n",
      "Epoch [400/500], Loss: 1.0820413827896118\n",
      "Epoch [500/500], Loss: 0.9363173246383667\n",
      "Epoch [100/500], Loss: 1.1258776187896729\n",
      "Epoch [200/500], Loss: 1.0283527374267578\n",
      "Epoch [300/500], Loss: 0.9620280861854553\n",
      "Epoch [400/500], Loss: 1.0107512474060059\n",
      "Epoch [500/500], Loss: 0.9770565032958984\n",
      "Epoch [100/500], Loss: 0.978107213973999\n",
      "Epoch [200/500], Loss: 0.9724088907241821\n",
      "Epoch [300/500], Loss: 1.0107553005218506\n",
      "Epoch [400/500], Loss: 1.071704626083374\n",
      "Epoch [500/500], Loss: 1.0125432014465332\n",
      "Epoch [100/500], Loss: 0.9275109171867371\n",
      "Epoch [200/500], Loss: 1.0592963695526123\n",
      "Epoch [300/500], Loss: 1.0286540985107422\n",
      "Epoch [400/500], Loss: 0.9800169467926025\n",
      "Epoch [500/500], Loss: 1.069223165512085\n",
      "Epoch [100/500], Loss: 1.095368504524231\n",
      "Epoch [200/500], Loss: 1.033923625946045\n",
      "Epoch [300/500], Loss: 0.9287196397781372\n",
      "Epoch [400/500], Loss: 1.1083372831344604\n",
      "Epoch [500/500], Loss: 1.0550695657730103\n",
      "Epoch [100/500], Loss: 1.05803382396698\n",
      "Epoch [200/500], Loss: 1.0319565534591675\n",
      "Epoch [300/500], Loss: 1.1349165439605713\n",
      "Epoch [400/500], Loss: 0.9954970479011536\n",
      "Epoch [500/500], Loss: 1.0557023286819458\n",
      "Epoch [100/500], Loss: 1.1413177251815796\n",
      "Epoch [200/500], Loss: 1.0731840133666992\n",
      "Epoch [300/500], Loss: 0.9712837934494019\n",
      "Epoch [400/500], Loss: 1.0443336963653564\n",
      "Epoch [500/500], Loss: 1.071901798248291\n",
      "Epoch [100/500], Loss: 1.0756984949111938\n",
      "Epoch [200/500], Loss: 1.1370192766189575\n",
      "Epoch [300/500], Loss: 1.009204626083374\n",
      "Epoch [400/500], Loss: 1.0756632089614868\n",
      "Epoch [500/500], Loss: 1.1306140422821045\n",
      "Epoch [100/500], Loss: 1.0505832433700562\n",
      "Epoch [200/500], Loss: 1.1985048055648804\n",
      "Epoch [300/500], Loss: 1.134352445602417\n",
      "Epoch [400/500], Loss: 1.097327709197998\n",
      "Epoch [500/500], Loss: 1.1153696775436401\n",
      "Epoch [100/500], Loss: 1.1012312173843384\n",
      "Epoch [200/500], Loss: 1.1259344816207886\n",
      "Epoch [300/500], Loss: 1.0040967464447021\n",
      "Epoch [400/500], Loss: 1.163029670715332\n",
      "Epoch [500/500], Loss: 1.1751233339309692\n",
      "Epoch [100/500], Loss: 1.0557950735092163\n",
      "Epoch [200/500], Loss: 1.0290478467941284\n",
      "Epoch [300/500], Loss: 1.1307384967803955\n",
      "Epoch [400/500], Loss: 1.0059007406234741\n",
      "Epoch [500/500], Loss: 1.154006004333496\n",
      "Epoch [100/500], Loss: 1.0093806982040405\n",
      "Epoch [200/500], Loss: 0.9643310308456421\n",
      "Epoch [300/500], Loss: 1.1414501667022705\n",
      "Epoch [400/500], Loss: 1.2591819763183594\n",
      "Epoch [500/500], Loss: 1.071452021598816\n",
      "Epoch [100/500], Loss: 1.1152364015579224\n",
      "Epoch [200/500], Loss: 1.170306921005249\n",
      "Epoch [300/500], Loss: 1.2623224258422852\n",
      "Epoch [400/500], Loss: 1.18229341506958\n",
      "Epoch [500/500], Loss: 1.1754727363586426\n",
      "Epoch [100/500], Loss: 1.1710054874420166\n",
      "Epoch [200/500], Loss: 0.9862098693847656\n",
      "Epoch [300/500], Loss: 1.0779742002487183\n",
      "Epoch [400/500], Loss: 1.1049119234085083\n",
      "Epoch [500/500], Loss: 1.0907829999923706\n",
      "Epoch [100/500], Loss: 1.107797622680664\n",
      "Epoch [200/500], Loss: 1.097991943359375\n",
      "Epoch [300/500], Loss: 1.2689235210418701\n",
      "Epoch [400/500], Loss: 1.2171796560287476\n",
      "Epoch [500/500], Loss: 1.197855830192566\n",
      "Epoch [100/500], Loss: 1.1900420188903809\n",
      "Epoch [200/500], Loss: 1.239108920097351\n",
      "Epoch [300/500], Loss: 1.231614351272583\n",
      "Epoch [400/500], Loss: 1.1561206579208374\n",
      "Epoch [500/500], Loss: 1.307112693786621\n",
      "Epoch [100/500], Loss: 1.1296806335449219\n",
      "Epoch [200/500], Loss: 1.1470046043395996\n",
      "Epoch [300/500], Loss: 1.1740927696228027\n",
      "Epoch [400/500], Loss: 1.0678236484527588\n",
      "Epoch [500/500], Loss: 1.2190887928009033\n",
      "Epoch [100/500], Loss: 1.1605756282806396\n",
      "Epoch [200/500], Loss: 1.2325785160064697\n",
      "Epoch [300/500], Loss: 1.175654411315918\n",
      "Epoch [400/500], Loss: 1.1127508878707886\n",
      "Epoch [500/500], Loss: 1.1410043239593506\n",
      "Epoch [100/500], Loss: 1.161647915840149\n",
      "Epoch [200/500], Loss: 1.2219395637512207\n",
      "Epoch [300/500], Loss: 1.2302186489105225\n",
      "Epoch [400/500], Loss: 1.240007996559143\n",
      "Epoch [500/500], Loss: 1.0906987190246582\n",
      "Epoch [100/500], Loss: 1.1479867696762085\n",
      "Epoch [200/500], Loss: 1.2093217372894287\n",
      "Epoch [300/500], Loss: 1.2057101726531982\n",
      "Epoch [400/500], Loss: 1.2131539583206177\n",
      "Epoch [500/500], Loss: 1.204943299293518\n",
      "Epoch [100/500], Loss: 1.178567886352539\n",
      "Epoch [200/500], Loss: 1.2796244621276855\n",
      "Epoch [300/500], Loss: 1.2290841341018677\n",
      "Epoch [400/500], Loss: 1.2525080442428589\n",
      "Epoch [500/500], Loss: 1.2312167882919312\n",
      "Epoch [100/500], Loss: 1.2671020030975342\n",
      "Epoch [200/500], Loss: 1.3556278944015503\n",
      "Epoch [300/500], Loss: 1.1863293647766113\n",
      "Epoch [400/500], Loss: 1.308342456817627\n",
      "Epoch [500/500], Loss: 1.3053629398345947\n",
      "Epoch [100/500], Loss: 1.3214260339736938\n",
      "Epoch [200/500], Loss: 1.339703917503357\n",
      "Epoch [300/500], Loss: 1.274126648902893\n",
      "Epoch [400/500], Loss: 1.1794509887695312\n",
      "Epoch [500/500], Loss: 1.3662869930267334\n",
      "Epoch [100/500], Loss: 1.3598068952560425\n",
      "Epoch [200/500], Loss: 1.2859420776367188\n",
      "Epoch [300/500], Loss: 1.2610666751861572\n",
      "Epoch [400/500], Loss: 1.3715457916259766\n",
      "Epoch [500/500], Loss: 1.340721607208252\n",
      "Epoch [100/500], Loss: 1.5151417255401611\n",
      "Epoch [200/500], Loss: 1.2876412868499756\n",
      "Epoch [300/500], Loss: 1.4344899654388428\n",
      "Epoch [400/500], Loss: 1.3373210430145264\n",
      "Epoch [500/500], Loss: 1.3428380489349365\n",
      "Epoch [100/500], Loss: 1.2388737201690674\n",
      "Epoch [200/500], Loss: 1.3094775676727295\n",
      "Epoch [300/500], Loss: 1.239853024482727\n",
      "Epoch [400/500], Loss: 1.2839226722717285\n",
      "Epoch [500/500], Loss: 1.4086008071899414\n",
      "Epoch [100/500], Loss: 1.3096574544906616\n",
      "Epoch [200/500], Loss: 1.2651723623275757\n",
      "Epoch [300/500], Loss: 1.2861073017120361\n",
      "Epoch [400/500], Loss: 1.3770757913589478\n",
      "Epoch [500/500], Loss: 1.1894557476043701\n",
      "Epoch [100/500], Loss: 1.3405261039733887\n",
      "Epoch [200/500], Loss: 1.3086774349212646\n",
      "Epoch [300/500], Loss: 1.1839570999145508\n",
      "Epoch [400/500], Loss: 1.2991191148757935\n",
      "Epoch [500/500], Loss: 1.214417576789856\n",
      "Epoch [100/500], Loss: 1.3198938369750977\n",
      "Epoch [200/500], Loss: 1.4118149280548096\n",
      "Epoch [300/500], Loss: 1.463210940361023\n",
      "Epoch [400/500], Loss: 1.2796162366867065\n",
      "Epoch [500/500], Loss: 1.4069039821624756\n",
      "Epoch [100/500], Loss: 1.35970139503479\n",
      "Epoch [200/500], Loss: 1.3309931755065918\n",
      "Epoch [300/500], Loss: 1.3111358880996704\n",
      "Epoch [400/500], Loss: 1.440647006034851\n",
      "Epoch [500/500], Loss: 1.351928472518921\n",
      "Epoch [100/500], Loss: 1.232482671737671\n",
      "Epoch [200/500], Loss: 1.3319339752197266\n",
      "Epoch [300/500], Loss: 1.2544490098953247\n",
      "Epoch [400/500], Loss: 1.4960156679153442\n",
      "Epoch [500/500], Loss: 1.3933351039886475\n",
      "Epoch [100/500], Loss: 1.4022777080535889\n",
      "Epoch [200/500], Loss: 1.3196661472320557\n",
      "Epoch [300/500], Loss: 1.4910080432891846\n",
      "Epoch [400/500], Loss: 1.4623188972473145\n",
      "Epoch [500/500], Loss: 1.503678321838379\n",
      "Epoch [100/500], Loss: 1.3831431865692139\n",
      "Epoch [200/500], Loss: 1.3467953205108643\n",
      "Epoch [300/500], Loss: 1.1947789192199707\n",
      "Epoch [400/500], Loss: 1.4875417947769165\n",
      "Epoch [500/500], Loss: 1.3531544208526611\n",
      "Epoch [100/500], Loss: 1.4219319820404053\n",
      "Epoch [200/500], Loss: 1.4731554985046387\n",
      "Epoch [300/500], Loss: 1.2809937000274658\n",
      "Epoch [400/500], Loss: 1.412649154663086\n",
      "Epoch [500/500], Loss: 1.472938060760498\n",
      "Epoch [100/500], Loss: 1.4065226316452026\n",
      "Epoch [200/500], Loss: 1.3793309926986694\n",
      "Epoch [300/500], Loss: 1.5349401235580444\n",
      "Epoch [400/500], Loss: 1.4130308628082275\n",
      "Epoch [500/500], Loss: 1.401509404182434\n",
      "Epoch [100/500], Loss: 1.6454734802246094\n",
      "Epoch [200/500], Loss: 1.4368469715118408\n",
      "Epoch [300/500], Loss: 1.6267486810684204\n",
      "Epoch [400/500], Loss: 1.474947214126587\n",
      "Epoch [500/500], Loss: 1.56223464012146\n",
      "Epoch [100/500], Loss: 1.2362722158432007\n",
      "Epoch [200/500], Loss: 1.4567971229553223\n",
      "Epoch [300/500], Loss: 1.2070249319076538\n",
      "Epoch [400/500], Loss: 1.3657859563827515\n",
      "Epoch [500/500], Loss: 1.5366662740707397\n",
      "Epoch [100/500], Loss: 1.5841758251190186\n",
      "Epoch [200/500], Loss: 1.696392297744751\n",
      "Epoch [300/500], Loss: 1.490470290184021\n",
      "Epoch [400/500], Loss: 1.4976491928100586\n",
      "Epoch [500/500], Loss: 1.480065107345581\n",
      "Epoch [100/500], Loss: 1.3681936264038086\n",
      "Epoch [200/500], Loss: 1.3582115173339844\n",
      "Epoch [300/500], Loss: 1.53867769241333\n",
      "Epoch [400/500], Loss: 1.5582194328308105\n",
      "Epoch [500/500], Loss: 1.581831932067871\n",
      "Epoch [100/500], Loss: 1.3220593929290771\n",
      "Epoch [200/500], Loss: 1.4329701662063599\n",
      "Epoch [300/500], Loss: 1.354396939277649\n",
      "Epoch [400/500], Loss: 1.6545014381408691\n",
      "Epoch [500/500], Loss: 1.6152852773666382\n",
      "Epoch [100/500], Loss: 1.3203320503234863\n",
      "Epoch [200/500], Loss: 1.5530997514724731\n",
      "Epoch [300/500], Loss: 1.6004003286361694\n",
      "Epoch [400/500], Loss: 1.5594271421432495\n",
      "Epoch [500/500], Loss: 1.5376800298690796\n",
      "Epoch [100/500], Loss: 1.6442434787750244\n",
      "Epoch [200/500], Loss: 1.6332440376281738\n",
      "Epoch [300/500], Loss: 1.4805774688720703\n",
      "Epoch [400/500], Loss: 1.4722850322723389\n",
      "Epoch [500/500], Loss: 1.5492035150527954\n",
      "Epoch [100/500], Loss: 1.5321767330169678\n",
      "Epoch [200/500], Loss: 1.5898997783660889\n",
      "Epoch [300/500], Loss: 1.5932908058166504\n",
      "Epoch [400/500], Loss: 1.3995157480239868\n",
      "Epoch [500/500], Loss: 1.5453068017959595\n",
      "Epoch [100/500], Loss: 1.486454725265503\n",
      "Epoch [200/500], Loss: 1.4610017538070679\n",
      "Epoch [300/500], Loss: 1.5303280353546143\n",
      "Epoch [400/500], Loss: 1.5299185514450073\n",
      "Epoch [500/500], Loss: 1.5522918701171875\n",
      "Epoch [100/500], Loss: 1.479682445526123\n",
      "Epoch [200/500], Loss: 1.6997911930084229\n",
      "Epoch [300/500], Loss: 1.5915085077285767\n",
      "Epoch [400/500], Loss: 1.5898008346557617\n",
      "Epoch [500/500], Loss: 1.4265718460083008\n",
      "Epoch [100/500], Loss: 1.4617633819580078\n",
      "Epoch [200/500], Loss: 1.5114482641220093\n",
      "Epoch [300/500], Loss: 1.4827191829681396\n",
      "Epoch [400/500], Loss: 1.4450832605361938\n",
      "Epoch [500/500], Loss: 1.595991849899292\n",
      "Epoch [100/500], Loss: 1.3790425062179565\n",
      "Epoch [200/500], Loss: 1.6102540493011475\n",
      "Epoch [300/500], Loss: 1.368670105934143\n",
      "Epoch [400/500], Loss: 1.594766616821289\n",
      "Epoch [500/500], Loss: 1.6019103527069092\n",
      "Epoch [100/500], Loss: 1.630807638168335\n",
      "Epoch [200/500], Loss: 1.6053543090820312\n",
      "Epoch [300/500], Loss: 1.5360054969787598\n",
      "Epoch [400/500], Loss: 1.5745410919189453\n",
      "Epoch [500/500], Loss: 1.3557088375091553\n",
      "Epoch [100/500], Loss: 1.7943055629730225\n",
      "Epoch [200/500], Loss: 1.7056083679199219\n",
      "Epoch [300/500], Loss: 1.6848206520080566\n",
      "Epoch [400/500], Loss: 1.715834617614746\n",
      "Epoch [500/500], Loss: 1.6347942352294922\n",
      "Epoch [100/500], Loss: 1.617937684059143\n",
      "Epoch [200/500], Loss: 1.639432430267334\n",
      "Epoch [300/500], Loss: 1.613572120666504\n",
      "Epoch [400/500], Loss: 1.5379788875579834\n",
      "Epoch [500/500], Loss: 1.249391794204712\n",
      "Epoch [100/500], Loss: 1.5773415565490723\n",
      "Epoch [200/500], Loss: 1.6341192722320557\n",
      "Epoch [300/500], Loss: 1.6088902950286865\n",
      "Epoch [400/500], Loss: 1.3078182935714722\n",
      "Epoch [500/500], Loss: 1.8029519319534302\n",
      "Epoch [100/500], Loss: 1.7417809963226318\n",
      "Epoch [200/500], Loss: 1.6841627359390259\n",
      "Epoch [300/500], Loss: 1.6287864446640015\n",
      "Epoch [400/500], Loss: 1.794883370399475\n",
      "Epoch [500/500], Loss: 1.4912388324737549\n",
      "Epoch [100/500], Loss: 1.708911657333374\n",
      "Epoch [200/500], Loss: 1.6537996530532837\n",
      "Epoch [300/500], Loss: 1.6168557405471802\n",
      "Epoch [400/500], Loss: 1.5462849140167236\n",
      "Epoch [500/500], Loss: 1.5868346691131592\n",
      "Epoch [100/500], Loss: 1.7199771404266357\n",
      "Epoch [200/500], Loss: 1.8072309494018555\n",
      "Epoch [300/500], Loss: 1.7429510354995728\n",
      "Epoch [400/500], Loss: 1.808552861213684\n",
      "Epoch [500/500], Loss: 1.7069146633148193\n",
      "Epoch [100/500], Loss: 1.5836181640625\n",
      "Epoch [200/500], Loss: 1.526221513748169\n",
      "Epoch [300/500], Loss: 1.615476131439209\n",
      "Epoch [400/500], Loss: 1.8728840351104736\n",
      "Epoch [500/500], Loss: 1.668067455291748\n",
      "[('OLS', '0', 0.5481553077697754, array([-0.05641403,  0.2045066 , -0.2786684 ,  0.04258093, -0.13245595,\n",
      "       -0.02498904, -0.07341998,  0.11288518,  0.04367145, -0.03371753,\n",
      "       -0.01590342,  0.01629774, -0.03170216,  0.09621067,  0.1001188 ,\n",
      "        0.03801021,  0.01125071, -0.1082947 , -0.02605204,  0.05626064,\n",
      "       -0.06587858,  0.10503986,  0.04913665, -0.06172792,  0.10267869],\n",
      "      dtype=float32)), ('LASSO', 0.14, 0.49643316864967346, array([ 0.00265748,  0.00379308, -0.01080554, -0.00501802, -0.01012849,\n",
      "        0.00803512,  0.00490858, -0.00458679,  0.00455794, -0.00046592,\n",
      "        0.00798037, -0.00089777, -0.00018705,  0.00715321,  0.01159553,\n",
      "       -0.00954969,  0.00809008, -0.00762686, -0.00883492, -0.0040512 ,\n",
      "        0.00149983, -0.00089422,  0.00428544, -0.01033436,  0.01220852],\n",
      "      dtype=float32)), ('TGR-LASSO', 0.63, 0.5017151832580566, array([-0.00444171,  0.06519108, -0.15693629,  0.00107552, -0.00188694,\n",
      "       -0.00200137, -0.00322715, -0.0010806 ,  0.00298069, -0.00320471,\n",
      "        0.00224149, -0.00162017,  0.0004129 ,  0.00093259,  0.00188459,\n",
      "        0.00132949,  0.00082487, -0.00260798,  0.00042954,  0.00087768,\n",
      "       -0.00363696, -0.00036184,  0.00092533, -0.00057184,  0.01958526],\n",
      "      dtype=float32)), ('Arctan', 0.99, 0.9838511943817139, array([-0.18855655,  0.19008212, -0.1452947 ,  0.00453487,  0.09833892,\n",
      "       -0.07382722,  0.00686157, -0.10695691,  0.16884623,  0.0502335 ,\n",
      "        0.14290865, -0.19890872, -0.08713849, -0.03878646,  0.11214744,\n",
      "       -0.04009512,  0.19783749, -0.07283258,  0.15324761, -0.13560903,\n",
      "       -0.02930483, -0.10997124, -0.05681518,  0.15162124,  0.04617433],\n",
      "      dtype=float32)), ('TGR - Special', 0.99, 0.9838511943817139, array([-4.9167559e-03, -6.2763998e-03, -5.1974263e-03,  5.7990314e-04,\n",
      "       -3.0007097e-05, -4.9964907e-03,  1.5191598e-03,  3.0752048e-03,\n",
      "       -1.6309158e-03,  1.5059218e-03, -7.2273128e-03, -4.5399936e-03,\n",
      "        9.9569908e-04, -1.3596704e-03, -5.3852997e-03,  7.6429744e-05,\n",
      "        1.7138320e-04, -1.1092749e-03, -1.3691118e-03, -5.5605796e-04,\n",
      "        1.8041548e-03, -4.0211069e-04, -2.0384148e-03, -2.2421263e-03,\n",
      "       -3.0175835e-04], dtype=float32))]\n",
      "tensor([-0.1115,  0.1204, -0.3696,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000])\n",
      "OLS RMSE: 0.5481553077697754\n",
      "Lasso RMSE: 0.5714990496635437\n",
      "TGR Lasso RMSE: 0.5072827935218811\n",
      "TGR RMSE: 0.5205656886100769\n",
      "Arctan Loss: 0.49631211161613464\n"
     ]
    }
   ],
   "source": [
    "##### PREDICTION SCRIPT\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tgr as tgr\n",
    "import log_hyperu as hyperu\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(123) # 42\n",
    "np.random.seed(123) # 42\n",
    "\n",
    "# Step 1: Generate synthetic data\n",
    "n_samples = 100\n",
    "n_features = 25\n",
    "n_nonzero = 3\n",
    "n_epochs = 500\n",
    "lambda_grid = [0.01*i for i in range(0,100,1)]\n",
    "\n",
    "output_list = list() # Contains Tuples with (METHOD, LAMBDA, LOSS, COEFS)\n",
    "\n",
    "# True coefficients with sparsity (many coefficients are zero)\n",
    "true_coefficients = torch.zeros(n_features)\n",
    "true_coefficients[:n_nonzero] = torch.randn(n_nonzero)\n",
    "\n",
    "# Generate features\n",
    "X = torch.randn(n_samples, n_features)\n",
    "\n",
    "# Generate targets with noise\n",
    "noise = torch.randn(n_samples) * 0.5\n",
    "y = X @ true_coefficients + noise\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2.5: Find optimal lambda for lasso\n",
    "#grid_search.fit(X_train, y_train)\n",
    "#optimal_lambda = grid_search.best_params_['alpha']\n",
    "#print(f\"Optimal lambda for LASSO: {optimal_lambda}\")\n",
    "\n",
    "# Step 3: Implement OLS and Lasso regression using PyTorch\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_features, 1, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "def train_model(model, X_train, y_train, lr=0.01, n_epochs=1000):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train).squeeze()\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return model\n",
    "\n",
    "# Train OLS model\n",
    "ols_model = LinearRegression(n_features)\n",
    "ols_model = train_model(ols_model, X_train, y_train)\n",
    "ols_loss = ComputeValidationLoss(ols_model, X_test, y_test)\n",
    "\n",
    "output_list.append((\"OLS\",\"0\",ols_loss,ols_model.linear.weight.detach().numpy().flatten()))\n",
    "# Train Lasso model\n",
    "best_loss = 10000000000 # Hard Gecoded!\n",
    "for lasso_reg_strength in lambda_grid:\n",
    "    lasso_model = LinearRegression(n_features)\n",
    "    def lasso_loss(output, target, model, lasso_reg_strength):\n",
    "        mse_loss = nn.MSELoss()(output, target)\n",
    "        lasso_loss = lasso_reg_strength * torch.norm(model.linear.weight, 1)\n",
    "        return mse_loss + lasso_loss\n",
    "\n",
    "    optimizer = torch.optim.SGD(lasso_model.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        lasso_model.train()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = lasso_model(X_train).squeeze()\n",
    "        loss = lasso_loss(outputs, y_train, lasso_model, lasso_reg_strength)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    val_loss = ComputeValidationLoss(lasso_model, X_test, y_test)\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_lamda = lasso_reg_strength\n",
    "        best_loss = val_loss\n",
    "        best_model = lasso_model\n",
    "\n",
    "output_list.append((\"LASSO\",best_lamda,best_loss,best_model.linear.weight.detach().numpy().flatten()))\n",
    "\n",
    "# Train TGR Model\n",
    "best_loss = 10000000000 # Hard Gecoded!\n",
    "for tgr_reg_strength in lambda_grid:\n",
    "    tgr_model1_LASSO = LinearRegression(n_features)\n",
    "    def tgr_loss(output, target, model, tgr_reg_strength, a, c, kappa):\n",
    "        mse_loss = nn.MSELoss()(output, target)\n",
    "        phi = torch.tensor((2*c)/((kappa**2)*a))\n",
    "        tgr_loss = tgr_reg_strength * torch.sum(-hyperu.log_hyperu(torch.tensor([[c+0.5]]),torch.tensor([[1.5-a]]),(model.linear.weight**2)/(2*phi))+hyperu.log_hyperu(torch.tensor([[c+0.5]]),torch.tensor([[1.5-a]]),torch.tensor([[0.0]])))\n",
    "\n",
    "        return mse_loss + tgr_loss\n",
    "    \n",
    "    optimizer = torch.optim.SGD(tgr_model1_LASSO.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        tgr_model1_LASSO.train()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = tgr_model1_LASSO(X_train).squeeze()\n",
    "        loss = tgr_loss(outputs, y_train, tgr_model1_LASSO, 0.25, 1, 30, 1)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(tgr_model1_LASSO.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    val_loss = ComputeValidationLoss(tgr_model1_LASSO, X_test, y_test)\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_lamda = tgr_reg_strength\n",
    "        best_loss = val_loss\n",
    "        best_model = tgr_model1_LASSO\n",
    "    \n",
    "output_list.append((\"TGR-LASSO\",best_lamda,best_loss,best_model.linear.weight.detach().numpy().flatten()))\n",
    "\n",
    "# Arctan Model\n",
    "best_loss = 10000000000 # Hard Gecoded!\n",
    "for arctan_reg_strength in lambda_grid:\n",
    "    arctan_model = LinearRegression(n_features)\n",
    "    def arctan_loss(output, target, model, arctan_reg_strength):\n",
    "        mse_loss = nn.MSELoss()(output, target)\n",
    "        arctan_loss = arctan_reg_strength*torch.sum((2/np.pi) * torch.arctan(torch.abs(model.linear.weight)))\n",
    "        return mse_loss + arctan_loss\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        arctan_model.train()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = arctan_model(X_train).squeeze()\n",
    "        loss = arctan_loss(outputs, y_train, arctan_model, arctan_reg_strength)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(arctan_model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item()}')\n",
    "        \n",
    "    val_loss = ComputeValidationLoss(arctan_model, X_test, y_test)\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_lamda = arctan_reg_strength\n",
    "        best_loss = val_loss\n",
    "        best_model = arctan_model\n",
    "    \n",
    "output_list.append((\"Arctan\",best_lamda,best_loss,best_model.linear.weight.detach().numpy().flatten()))\n",
    "\n",
    "        \n",
    "## TGR - Special\n",
    "tgr_model2_Special = LinearRegression(n_features)\n",
    "best_loss = 10000000000 # Hard Gecoded!\n",
    "for tgr_reg_strength in lambda_grid:\n",
    "    def tgr_loss(output, target, model, tgr_reg_strength, a, c, kappa):\n",
    "        mse_loss = nn.MSELoss()(output, target)\n",
    "        phi = torch.tensor((2*c)/((kappa**2)*a))\n",
    "        tgr_loss = tgr_reg_strength * torch.sum(-hyperu.log_hyperu(torch.tensor([[c+0.5]]),torch.tensor([[1.5-a]]),(model.linear.weight**2)/(2*phi))+hyperu.log_hyperu(torch.tensor([[c+0.5]]),torch.tensor([[1.5-a]]),torch.tensor([[0.0]])))\n",
    "\n",
    "        return mse_loss + tgr_loss\n",
    "\n",
    "    optimizer = torch.optim.SGD(tgr_model2_Special.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        tgr_model2_Special.train()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = tgr_model2_Special(X_train).squeeze()\n",
    "        loss = tgr_loss(outputs, y_train, tgr_model2_Special, tgr_reg_strength, 0.75, 20, 2)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(tgr_model2_Special.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_lamda = tgr_reg_strength\n",
    "        best_loss = val_loss\n",
    "        best_model = tgr_model2_Special\n",
    "\n",
    "output_list.append((\"TGR - Special\",best_lamda,best_loss,best_model.linear.weight.detach().numpy().flatten()))\n",
    "\n",
    "\n",
    "print(f\"OLS RMSE: {ols_rmse}\")\n",
    "print(f\"Lasso RMSE: {lasso_rmse}\")\n",
    "print(f\"TGR Lasso RMSE: {tgr_LASSO_rmse}\")\n",
    "print(f\"TGR RMSE: {tgr_rmse}\")\n",
    "print(f\"Arctan Loss: {arctan_rmse}\")\n",
    "\n",
    "# Step 5: Plot the true and estimated coefficients\n",
    "\n",
    "# Get the estimated coefficients\n",
    "#ols_coefficients = ols_model.linear.weight.detach().numpy().flatten()\n",
    "#lasso_coefficients = lasso_model.linear.weight.detach().numpy().flatten()\n",
    "#tgr_coefficients_LASSO = tgr_model1_LASSO.linear.weight.detach().numpy().flatten()\n",
    "#tgr_coefficients_Special = tgr_model2_Special.linear.weight.detach().numpy().flatten()\n",
    "#arctan_coefficients = arctan_model.linear.weight.detach().numpy().flatten()\n",
    "#true_coefficients_np = true_coefficients.numpy()\n",
    "\n",
    "# Plotting\n",
    "#fig = plt.figure(figsize=(12, 6))\n",
    "#ax = fig.add_subplot(1, 1, 1)\n",
    "#plt.plot(true_coefficients_np, label='True Coefficients', marker='o')\n",
    "#plt.plot(ols_coefficients, label='OLS Estimated', marker='x')\n",
    "#plt.plot(lasso_coefficients, label='LASSO Estimated', marker='.')\n",
    "#plt.plot(tgr_coefficients_LASSO, label='TGR Estimated - LASSO Replica', marker='.')\n",
    "#plt.plot(tgr_coefficients_Special, label='TGR Estimated - Example', marker='.')\n",
    "#plt.plot(arctan_coefficients, label='Arctan Estimated', marker='.')\n",
    "#plt.xlabel('Feature Index')\n",
    "#plt.ylabel('Coefficient Value')\n",
    "#plt.title('True vs Estimated Coefficients')\n",
    "\n",
    "#major_ticks = np.arange(0, 10, 1)\n",
    "#ax.set_xticks(major_ticks)\n",
    "\n",
    "#plt.legend()\n",
    "#plt.grid(True)\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run 0 of 100\n",
      "Finished run 1 of 100\n",
      "Finished run 2 of 100\n",
      "Finished run 3 of 100\n",
      "Finished run 4 of 100\n",
      "Finished run 5 of 100\n",
      "Finished run 6 of 100\n",
      "Finished run 7 of 100\n",
      "Finished run 8 of 100\n",
      "Finished run 9 of 100\n",
      "Finished run 10 of 100\n",
      "Finished run 11 of 100\n",
      "Finished run 12 of 100\n",
      "Finished run 13 of 100\n",
      "Finished run 14 of 100\n",
      "Finished run 15 of 100\n",
      "Finished run 16 of 100\n",
      "Finished run 17 of 100\n",
      "Finished run 18 of 100\n",
      "Finished run 19 of 100\n",
      "Finished run 20 of 100\n",
      "Finished run 21 of 100\n",
      "Finished run 22 of 100\n",
      "Finished run 23 of 100\n",
      "Finished run 24 of 100\n",
      "Finished run 25 of 100\n",
      "Finished run 26 of 100\n",
      "Finished run 27 of 100\n",
      "Finished run 28 of 100\n",
      "Finished run 29 of 100\n",
      "Finished run 30 of 100\n",
      "Finished run 31 of 100\n",
      "Finished run 32 of 100\n",
      "Finished run 33 of 100\n",
      "Finished run 34 of 100\n",
      "Finished run 35 of 100\n",
      "Finished run 36 of 100\n",
      "Finished run 37 of 100\n",
      "Finished run 38 of 100\n",
      "Finished run 39 of 100\n",
      "Finished run 40 of 100\n",
      "Finished run 41 of 100\n",
      "Finished run 42 of 100\n",
      "Finished run 43 of 100\n",
      "Finished run 44 of 100\n",
      "Finished run 45 of 100\n",
      "Finished run 46 of 100\n",
      "Finished run 47 of 100\n",
      "Finished run 48 of 100\n",
      "Finished run 49 of 100\n",
      "Finished run 50 of 100\n",
      "Finished run 51 of 100\n",
      "Finished run 52 of 100\n",
      "Finished run 53 of 100\n",
      "Finished run 54 of 100\n",
      "Finished run 55 of 100\n",
      "Finished run 56 of 100\n",
      "Finished run 57 of 100\n",
      "Finished run 58 of 100\n",
      "Finished run 59 of 100\n",
      "Finished run 60 of 100\n",
      "Finished run 61 of 100\n",
      "Finished run 62 of 100\n",
      "Finished run 63 of 100\n",
      "Finished run 64 of 100\n",
      "Finished run 65 of 100\n",
      "Finished run 66 of 100\n",
      "Finished run 67 of 100\n",
      "Finished run 68 of 100\n",
      "Finished run 69 of 100\n",
      "Finished run 70 of 100\n",
      "Finished run 71 of 100\n",
      "Finished run 72 of 100\n",
      "Finished run 73 of 100\n",
      "Finished run 74 of 100\n",
      "Finished run 75 of 100\n",
      "Finished run 76 of 100\n",
      "Finished run 77 of 100\n",
      "Finished run 78 of 100\n",
      "Finished run 79 of 100\n",
      "Finished run 80 of 100\n",
      "Finished run 81 of 100\n",
      "Finished run 82 of 100\n",
      "Finished run 83 of 100\n",
      "Finished run 84 of 100\n",
      "Finished run 85 of 100\n",
      "Finished run 86 of 100\n",
      "Finished run 87 of 100\n",
      "Finished run 88 of 100\n",
      "Finished run 89 of 100\n",
      "Finished run 90 of 100\n",
      "Finished run 91 of 100\n",
      "Finished run 92 of 100\n",
      "Finished run 93 of 100\n",
      "Finished run 94 of 100\n",
      "Finished run 95 of 100\n",
      "Finished run 96 of 100\n",
      "Finished run 97 of 100\n",
      "Finished run 98 of 100\n",
      "Finished run 99 of 100\n"
     ]
    }
   ],
   "source": [
    "### Absolut Deviation Plot\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(123) # 42\n",
    "np.random.seed(123) # 42\n",
    "\n",
    "# Step 1: Generate synthetic data\n",
    "n_samples = 100\n",
    "n_features = 10\n",
    "n_nonzero = 3\n",
    "runs = 100\n",
    "\n",
    "ols_dev_list = list()\n",
    "lasso_dev_list = list()\n",
    "tgr_dev_list = list()\n",
    "\n",
    "for run in range(runs):\n",
    "    # True coefficients with sparsity (many coefficients are zero)\n",
    "    true_coefficients = torch.zeros(n_features)\n",
    "    true_coefficients[:n_nonzero] = torch.randn(n_nonzero)\n",
    "\n",
    "    # Generate features\n",
    "    X = torch.randn(n_samples, n_features)\n",
    "\n",
    "    # Generate targets with noise\n",
    "    noise = torch.randn(n_samples) * 0.5\n",
    "    y = X @ true_coefficients + noise\n",
    "\n",
    "    # Step 2: Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Step 3: Implement OLS and Lasso regression using PyTorch\n",
    "    class LinearRegression(nn.Module):\n",
    "        def __init__(self, n_features):\n",
    "            super(LinearRegression, self).__init__()\n",
    "            self.linear = nn.Linear(n_features, 1, bias=False)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            return self.linear(x)\n",
    "\n",
    "    def train_model(model, X_train, y_train, lr=0.01, n_epochs=1000):\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            model.train()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_train).squeeze()\n",
    "            loss = criterion(outputs, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        return model\n",
    "\n",
    "    # Train OLS model\n",
    "    ols_model = LinearRegression(n_features)\n",
    "    ols_model = train_model(ols_model, X_train, y_train)\n",
    "\n",
    "    # Train Lasso model\n",
    "    lasso_model = LinearRegression(n_features)\n",
    "    lasso_reg_strength = 0.25 # Regularization strength\n",
    "\n",
    "    def lasso_loss(output, target, model, lasso_reg_strength):\n",
    "        mse_loss = nn.MSELoss()(output, target)\n",
    "        lasso_loss = lasso_reg_strength * torch.norm(model.linear.weight, 1)\n",
    "        return mse_loss + lasso_loss\n",
    "\n",
    "    optimizer = torch.optim.SGD(lasso_model.parameters(), lr=0.01)\n",
    "    n_epochs = 1000\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        lasso_model.train()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = lasso_model(X_train).squeeze()\n",
    "        loss = lasso_loss(outputs, y_train, lasso_model, lasso_reg_strength)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Train TGR Model\n",
    "    tgr_model2_Special = LinearRegression(n_features)\n",
    "\n",
    "    def tgr_loss(output, target, model, tgr_reg_strength, a, c, kappa):\n",
    "        mse_loss = nn.MSELoss()(output, target)\n",
    "        phi = torch.tensor((2*c)/((kappa**2)*a))\n",
    "        tgr_loss = tgr_reg_strength * torch.sum(-hyperu.log_hyperu(torch.tensor([[c+0.5]]),torch.tensor([[1.5-a]]),(model.linear.weight**2)/(2*phi))+hyperu.log_hyperu(torch.tensor([[c+0.5]]),torch.tensor([[1.5-a]]),torch.tensor([[0.0]])))\n",
    "\n",
    "        return mse_loss + tgr_loss\n",
    "\n",
    "    ## TGR - Special\n",
    "    optimizer = torch.optim.SGD(tgr_model2_Special.parameters(), lr=0.01)\n",
    "    n_epochs = 1000\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        tgr_model2_Special.train()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = tgr_model2_Special(X_train).squeeze()\n",
    "        loss = tgr_loss(outputs, y_train, tgr_model2_Special, 0.05, 0.75, 20, 2)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(tgr_model2_Special.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    # Step 5: Plot the true and estimated coefficients\n",
    "\n",
    "    # Get the estimated coefficients\n",
    "    ols_coefficients = ols_model.linear.weight.detach().numpy().flatten()\n",
    "    lasso_coefficients = lasso_model.linear.weight.detach().numpy().flatten()\n",
    "    tgr_coefficients_Special = tgr_model2_Special.linear.weight.detach().numpy().flatten()\n",
    "    true_coefficients_np = true_coefficients.numpy()\n",
    "\n",
    "    # Get absolute deviation\n",
    "    ols_dev_list.append(sum(abs(ols_coefficients-true_coefficients_np)))\n",
    "    lasso_dev_list.append(sum(abs(lasso_coefficients-true_coefficients_np)))\n",
    "    tgr_dev_list.append(sum(abs(tgr_coefficients_Special - true_coefficients_np)))\n",
    "    \n",
    "    print(f'Finished run {run} of {runs}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(ols_dev_list)\n",
    "#print(lasso_dev_list)\n",
    "#print(tgr_dev_list)\n",
    "\n",
    "# Combine data into a list\n",
    "data = [ols_dev_list, lasso_dev_list, tgr_dev_list]\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Plot box plots\n",
    "ax.boxplot(data, patch_artist=True, notch=True,\n",
    "            boxprops=dict(facecolor='skyblue', color='blue'),\n",
    "            capprops=dict(color='blue'),\n",
    "            whiskerprops=dict(color='blue'),\n",
    "            flierprops=dict(color='blue', markeredgecolor='blue'),\n",
    "            medianprops=dict(color='red'))\n",
    "plt.title('Distribution of the Sum of the Absolute Deviations \\nof the Estimates to the True Coefficients')\n",
    "ax.set_xlabel('Regularization Method')\n",
    "ax.set_ylabel('Absolute Deviation')\n",
    "ax.set_xticklabels(['OLS Deviation', 'LASSO Deviation', 'TGR Deviation'])\n",
    "major_ticks = np.arange(1, 4, 1)\n",
    "ax.set_xticks(major_ticks)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
