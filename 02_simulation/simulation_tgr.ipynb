{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Libraries\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import log_hyperu as hyperu\n",
    "import tgr as tgr\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the data sets\n",
    "\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "# Structure of the data sets\n",
    "## Set X_A: Sparse Coefficients with many data points\n",
    "## Set X_B: Dense Coefficients with many data points\n",
    "## Set X_C: Sparse Coefficients with few data points\n",
    "## Set X_D: Dense Coefficients with few data points\n",
    "\n",
    "\n",
    "### Set X_A\n",
    "variables = 10\n",
    "sample = 100\n",
    "true_coefs = torch.tensor([[0],[0],[5.3],[4.2],[0],[0],[6.9],[0],[0],[0]])\n",
    "X = torch.randn(sample, variables)\n",
    "Y = X @ true_coefs + torch.randn(sample, 1) * 0.1\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1500], Loss: 111.72769165039062\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Lucas Paul\\Documents\\GitHub_Repos\\MScThesis_Econ\\02_simulation\\simulation_tgr.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lucas%20Paul/Documents/GitHub_Repos/MScThesis_Econ/02_simulation/simulation_tgr.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m iterations \u001b[39m=\u001b[39m \u001b[39m1500\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lucas%20Paul/Documents/GitHub_Repos/MScThesis_Econ/02_simulation/simulation_tgr.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m starttime \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Lucas%20Paul/Documents/GitHub_Repos/MScThesis_Econ/02_simulation/simulation_tgr.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m trained_model2, coefs, loss_of_optimization \u001b[39m=\u001b[39m tgr\u001b[39m.\u001b[39;49mTripleGammaModel(X, Y, \u001b[39m1\u001b[39;49m, \u001b[39m0.51\u001b[39;49m, \u001b[39m0.001\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m, num_epochs\u001b[39m=\u001b[39;49miterations) \u001b[39m# Covariates, Targets, Penalty, a, c, kappa, normalization=True\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lucas%20Paul/Documents/GitHub_Repos/MScThesis_Econ/02_simulation/simulation_tgr.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m endtime \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lucas%20Paul/Documents/GitHub_Repos/MScThesis_Econ/02_simulation/simulation_tgr.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mWith \u001b[39m\u001b[39m{\u001b[39;00miterations\u001b[39m}\u001b[39;00m\u001b[39m iterations, TG Regularization took \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mround\u001b[39m(endtime\u001b[39m-\u001b[39mstarttime,\u001b[39m4\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m seconds. On this system, that is roughly \u001b[39m\u001b[39m{\u001b[39;00m(endtime\u001b[39m-\u001b[39mstarttime)\u001b[39m/\u001b[39miterations\u001b[39m}\u001b[39;00m\u001b[39m seconds per iteration.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Lucas Paul\\Documents\\GitHub_Repos\\MScThesis_Econ\\02_simulation\\tgr.py:47\u001b[0m, in \u001b[0;36mTripleGammaModel\u001b[1;34m(X, y, penalty, a, c, kappa, norm, num_epochs, lr)\u001b[0m\n\u001b[0;32m     45\u001b[0m outputs \u001b[39m=\u001b[39m model(X)\n\u001b[0;32m     46\u001b[0m loss \u001b[39m=\u001b[39m TripleGammaReg_Loss(outputs, y, model\u001b[39m.\u001b[39mlinear\u001b[39m.\u001b[39mweight, penalty, a, c, kappa, norm)\n\u001b[1;32m---> 47\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     48\u001b[0m coef_list\u001b[39m.\u001b[39mappend(model\u001b[39m.\u001b[39mlinear\u001b[39m.\u001b[39mweight)\n\u001b[0;32m     49\u001b[0m loss_list\u001b[39m.\u001b[39mappend(loss)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\function.py:289\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    284\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mImplementing both \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbackward\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvjp\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for a custom \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    285\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFunction is not allowed. You should only implement one \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mof them.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    287\u001b[0m     )\n\u001b[0;32m    288\u001b[0m user_fn \u001b[39m=\u001b[39m vjp_fn \u001b[39mif\u001b[39;00m vjp_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Function\u001b[39m.\u001b[39mvjp \u001b[39melse\u001b[39;00m backward_fn\n\u001b[1;32m--> 289\u001b[0m \u001b[39mreturn\u001b[39;00m user_fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[1;32mc:\\Users\\Lucas Paul\\Documents\\GitHub_Repos\\MScThesis_Econ\\02_simulation\\log_hyperu.py:78\u001b[0m, in \u001b[0;36mlog_hyperu.backward\u001b[1;34m(ctx, grad_output)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(ctx, grad_output):\n\u001b[0;32m     77\u001b[0m     a, b, x, u_res \u001b[39m=\u001b[39m ctx\u001b[39m.\u001b[39msaved_tensors\n\u001b[1;32m---> 78\u001b[0m     grad_x \u001b[39m=\u001b[39m grad_output \u001b[39m*\u001b[39m (\u001b[39m-\u001b[39ma \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mdiv(robust_hyperu(a \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, b \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, x), u_res))\n\u001b[0;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, grad_x\n",
      "File \u001b[1;32mc:\\Users\\Lucas Paul\\Documents\\GitHub_Repos\\MScThesis_Econ\\02_simulation\\log_hyperu.py:52\u001b[0m, in \u001b[0;36mrobust_hyperu\u001b[1;34m(a, b, z)\u001b[0m\n\u001b[0;32m     47\u001b[0m res \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(torch\u001b[39m.\u001b[39mlogical_and(small_z, b \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m),\n\u001b[0;32m     48\u001b[0m                     torch\u001b[39m.\u001b[39mexp(torch\u001b[39m.\u001b[39mlgamma(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m b) \u001b[39m-\u001b[39m torch\u001b[39m.\u001b[39mlgamma(a \u001b[39m-\u001b[39m b \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)),\n\u001b[0;32m     49\u001b[0m                     res)\n\u001b[0;32m     51\u001b[0m \u001b[39m# Fill up all where no special case applies\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m res \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(torch\u001b[39m.\u001b[39misnan(res), sp\u001b[39m.\u001b[39;49mhyperu(a, b, z), res)\n\u001b[0;32m     54\u001b[0m \u001b[39mreturn\u001b[39;00m(res)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:1068\u001b[0m, in \u001b[0;36mTensor.__array_wrap__\u001b[1;34m(self, array)\u001b[0m\n\u001b[0;32m   1064\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   1066\u001b[0m \u001b[39m# Wrap Numpy array again in a suitable tensor when done, to support e.g.\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m \u001b[39m# `numpy.sin(tensor) -> tensor` or `numpy.greater(tensor, 0) -> ByteTensor`\u001b[39;00m\n\u001b[1;32m-> 1068\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array_wrap__\u001b[39m(\u001b[39mself\u001b[39m, array):\n\u001b[0;32m   1069\u001b[0m     \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1070\u001b[0m         \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   1071\u001b[0m             Tensor\u001b[39m.\u001b[39m__array_wrap__, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, array\u001b[39m=\u001b[39marray\n\u001b[0;32m   1072\u001b[0m         )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iterations = 1500\n",
    "starttime = time.time()\n",
    "trained_model2, coefs, loss_of_optimization = tgr.TripleGammaModel(X, Y, 1, 0.51, 0.001, 2, True, num_epochs=iterations) # Covariates, Targets, Penalty, a, c, kappa, normalization=True\n",
    "endtime = time.time()\n",
    "print(f'With {iterations} iterations, TG Regularization took {round(endtime-starttime,4)} seconds. On this system, that is roughly {(endtime-starttime)/iterations} seconds per iteration.')\n",
    "coefficients = trained_model2.linear.weight.detach().numpy()\n",
    "#intercept = trained_model2.linear.bias.item()\n",
    "\n",
    "# For LASSO Imitation: (X, Y, 1, 1, 30, 1, True, num_epochs=2000) # Covariates, Targets, Penalty, a, c, kappa, normalization=True\n",
    "\n",
    "#print(\"Coefficients General Function:\", coefficients)\n",
    "#print(\"Intercept General Function:\", intercept)\n",
    "print(coefficients[0].tolist())\n",
    "true_coefs_TGR = coefficients[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 10.6036\n",
      "Epoch [200/1000], Loss: 7.0338\n",
      "Epoch [300/1000], Loss: 6.6343\n",
      "Epoch [400/1000], Loss: 6.5833\n",
      "Epoch [500/1000], Loss: 6.5743\n",
      "Epoch [600/1000], Loss: 6.5747\n",
      "Epoch [700/1000], Loss: 6.5778\n",
      "Epoch [800/1000], Loss: 6.5804\n",
      "Epoch [900/1000], Loss: 6.5821\n",
      "Epoch [1000/1000], Loss: 6.5785\n",
      "With 1000 iterations, Regular LASSO Regularization took 1.3717 seconds. On this system, that is roughly 0.0013716657161712646 seconds per iteration.\n",
      "linear.weight: [[-0.08131898939609528, 0.11880242824554443, 5.130025863647461, 4.099651336669922, 0.0024799692910164595, -0.008265404962003231, 6.229494094848633, -0.03251353278756142, 0.006142516154795885, -0.11067549884319305]]\n"
     ]
    }
   ],
   "source": [
    "#LASSO\n",
    "# Define your model\n",
    "starttime = time.time()\n",
    "class LinearRegressionLasso(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegressionLasso, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = 10\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "lambda_lasso = 1 # L1 regularization parameter\n",
    "\n",
    "# Create the model\n",
    "model = LinearRegressionLasso(input_size, output_size)\n",
    "\n",
    "# Define loss function (MSE loss with L1 regularization)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, Y)\n",
    "\n",
    "    # Total loss with L1 regularization\n",
    "    l1_norm = sum(torch.linalg.norm(p, 1) for p in model.parameters())\n",
    "\n",
    "    loss = loss + lambda_lasso * l1_norm\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "endtime = time.time()\n",
    "print(f'With {num_epochs} iterations, Regular LASSO Regularization took {round(endtime-starttime,4)} seconds. On this system, that is roughly {(endtime-starttime)/num_epochs} seconds per iteration.')\n",
    "\n",
    "# After training, you can access the learned coefficients\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        print(f'{name}: {param.data.tolist()}')\n",
    "        coefs_LASSO = param.data.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CopmuteRMSE(Y, X, coefficients, method):\n",
    "    # Variable Type Check\n",
    "    if not all(isinstance(t, torch.Tensor) for t in [Y, X, coefficients]):\n",
    "        raise ValueError(\"Not all inputs are PyTorch tensors!\")\n",
    "    \n",
    "    Y_hat = X @ coefficients\n",
    "    Y = torch.squeeze(Y)\n",
    "    mse = torch.sum((Y - Y_hat)**2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "    print(method)\n",
    "    #print(Y_hat)\n",
    "    #print(Y)\n",
    "    print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "ols = linear_model.LinearRegression()\n",
    "model = ols.fit(X, Y)\n",
    "ols = model.coef_.tolist()[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True COEFS [0.0, 0.0, 5.300000190734863, 4.199999809265137, 0.0, 0.0, 6.900000095367432, 0.0, 0.0, 0.0]\n",
      "TGR: [0.005852716509252787, -0.003165281843394041, 5.298001289367676, 4.181629180908203, -0.006983057130128145, 2.2076303139328957e-05, 6.898454666137695, -0.008982997387647629, -0.00298516359180212, -0.00657022837549448]\n",
      "LASSO: [-0.08131898939609528, 0.11880242824554443, 5.130025863647461, 4.099651336669922, 0.0024799692910164595, -0.008265404962003231, 6.229494094848633, -0.03251353278756142, 0.006142516154795885, -0.11067549884319305]\n",
      "OLS: [-0.011616242118179798, 0.005331128835678101, 5.29803991317749, 4.181795120239258, -0.013096123933792114, 0.003976382315158844, 6.89629602432251, -0.014410754665732384, -0.0016365605406463146, -0.008706651628017426]\n",
      "\n",
      "\n",
      "TGR\n",
      "tensor(0.9780)\n",
      "LASSO\n",
      "tensor(5.8565)\n",
      "OLS\n",
      "tensor(0.9644)\n"
     ]
    }
   ],
   "source": [
    "print(\"True COEFS\", torch.squeeze(true_coefs).T.tolist())\n",
    "print(\"TGR:\",torch.tensor(true_coefs_TGR).tolist())\n",
    "print(\"LASSO:\",torch.squeeze(torch.tensor(coefs_LASSO)).tolist())\n",
    "print(\"OLS:\", ols)\n",
    "print(\"\\n\")\n",
    "CopmuteRMSE(Y,X,torch.tensor(true_coefs_TGR), \"TGR\")\n",
    "CopmuteRMSE(Y,X,torch.squeeze(torch.tensor(coefs_LASSO)), \"LASSO\")\n",
    "CopmuteRMSE(Y,X,torch.squeeze(torch.tensor(ols)), \"OLS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 5.327718734741211\n",
      "Epoch [200/1000], Loss: 5.111759185791016\n",
      "Epoch [300/1000], Loss: 4.500098705291748\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tgr as tgr\n",
    "import log_hyperu as hyperu\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Step 1: Generate synthetic data\n",
    "n_samples = 100\n",
    "n_features = 10\n",
    "\n",
    "# True coefficients with sparsity (many coefficients are zero)\n",
    "true_coefficients = torch.zeros(n_features)\n",
    "true_coefficients[:3] = torch.randn(3)\n",
    "\n",
    "# Generate features\n",
    "X = torch.randn(n_samples, n_features)\n",
    "\n",
    "# Generate targets with noise\n",
    "noise = torch.randn(n_samples) * 0.5\n",
    "y = X @ true_coefficients + noise\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Implement OLS and Lasso regression using PyTorch\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_features, 1, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "def train_model(model, X_train, y_train, lr=0.01, n_epochs=1000):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train).squeeze()\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return model\n",
    "\n",
    "# Train OLS model\n",
    "ols_model = LinearRegression(n_features)\n",
    "ols_model = train_model(ols_model, X_train, y_train)\n",
    "\n",
    "# Train Lasso model\n",
    "lasso_model = LinearRegression(n_features)\n",
    "lasso_reg_strength = 0.5  # Regularization strength\n",
    "\n",
    "def lasso_loss(output, target, model, lasso_reg_strength):\n",
    "    mse_loss = nn.MSELoss()(output, target)\n",
    "    lasso_loss = lasso_reg_strength * torch.norm(model.linear.weight, 1)\n",
    "    return mse_loss + lasso_loss\n",
    "\n",
    "optimizer = torch.optim.SGD(lasso_model.parameters(), lr=0.01)\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    lasso_model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = lasso_model(X_train).squeeze()\n",
    "    loss = lasso_loss(outputs, y_train, lasso_model, lasso_reg_strength)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# Train TGR Model\n",
    "tgr_model = LinearRegression(n_features)\n",
    "tgr_reg_strength = 0.1\n",
    "\n",
    "def tgr_loss(output, target, model, lasso_reg_strength):\n",
    "    mse_loss = nn.MSELoss()(output, target)\n",
    "    c = 0.001\n",
    "    a = 0.51\n",
    "    kappa = 2\n",
    "    phi = torch.tensor((2*c)/((kappa**2)*a))\n",
    "    tgr_loss = tgr_reg_strength * torch.sum(-hyperu.log_hyperu(torch.tensor([[c+0.5]]),torch.tensor([[1.5-a]]),(model.linear.weight**2)/(2*phi))+hyperu.log_hyperu(torch.tensor([[c+0.5]]),torch.tensor([[1.5-a]]),torch.tensor([[0.0]])))\n",
    "\n",
    "    return mse_loss + tgr_loss\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(tgr_model.parameters(), lr=0.01)\n",
    "n_epochs = 1500\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    tgr_model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = tgr_model(X_train).squeeze()\n",
    "    loss = tgr_loss(outputs, y_train, tgr_model, lasso_reg_strength)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(tgr_model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "        \n",
    "# Step 4: Compare performance based on RMSE\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test).squeeze()\n",
    "    rmse = torch.sqrt(nn.MSELoss()(predictions, y_test))\n",
    "    return rmse.item()\n",
    "\n",
    "ols_rmse = evaluate_model(ols_model, X_test, y_test)\n",
    "lasso_rmse = evaluate_model(lasso_model, X_test, y_test)\n",
    "tgr_rmse = evaluate_model(tgr_model, X_test, y_test)\n",
    "\n",
    "print(f\"OLS RMSE: {ols_rmse}\")\n",
    "print(f\"Lasso RMSE: {lasso_rmse}\")\n",
    "print(f\"TGR RMSE: {tgr_rmse}\")\n",
    "\n",
    "# Step 5: Plot the true and estimated coefficients\n",
    "\n",
    "# Get the estimated coefficients\n",
    "ols_coefficients = ols_model.linear.weight.detach().numpy().flatten()\n",
    "lasso_coefficients = lasso_model.linear.weight.detach().numpy().flatten()\n",
    "tgr_coefficients = tgr_model.linear.weight.detach().numpy().flatten()\n",
    "true_coefficients_np = true_coefficients.numpy()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(true_coefficients_np, label='True Coefficients', marker='o')\n",
    "plt.plot(ols_coefficients, label='OLS Estimated Coefficients', marker='x')\n",
    "plt.plot(lasso_coefficients, label='Lasso Estimated Coefficients', marker='*')\n",
    "plt.plot(tgr_coefficients, label='TGR Estimated Coefficients', marker='.')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('True vs Estimated Coefficients')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
