\documentclass[12pt,a4paper]{article}
\usepackage[
	left 	= 2.54cm,
	right 	= 2.54cm, 
	top 		= 2.54cm,
	bottom 	= 2.54cm,
]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[OT1]{fontenc}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{lscape}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{amssymb}
\usepackage{tikz}

% Packages for Tables -----
\usepackage{multirow}
\usepackage{booktabs}
\usepackage[justification=centering]{caption} % Centers the caption
\usepackage{makecell}
\usepackage{lscape}
% -----

% Colorbox for Notes ------
\usepackage{tcolorbox}
\usepackage{xcolor}
\definecolor{lightblue}{RGB}{173,216,230}

\newenvironment{lightbluebox}{%
    \begin{tcolorbox}[colback=lightblue, colframe=lightblue, fontupper=\itshape]%
}{%
    \end{tcolorbox}%
}
%---------------------------

\usepackage[style=apa, backend=biber]{biblatex}
\addbibresource[]{ref.bib}
\renewcommand{\baselinestretch}{1.25} 

\usepackage[hidelinks]{hyperref}
\hypersetup{
	colorlinks = true,
	urlcolor   = blue,
	linkcolor  = black, 
	citecolor  = blue, 
}

\definecolor{green1}{HTML}{2CA02C}
\definecolor{blue1}{HTML}{1F77B4}
\definecolor{red1}{HTML}{D62728}
\definecolor{orange1}{HTML}{FF7F0E}


\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\slshape Unterweger}
\chead{}
\rhead{\slshape \nouppercase{\leftmark}}

\usepackage{titlesec,xcolor}
\titleformat{\section}{\bfseries}{\thesection}{0.5em}{}
\titlespacing{\section}{0pt}{3ex plus 1ex minus 0.2ex}{10pt}
\setlength{\headheight}{14.49998pt}

\usepackage{titlesec,xcolor}
\titleformat{\subsection}{\bfseries}{\thesubsection}{0.5em}{}
\titlespacing{\subsection}{0pt}{3ex plus 1ex minus 0.2ex}{10pt}
\setlength{\headheight}{14.49998pt}

%% Command Overrides
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\MC}{\textbf{(Missing Citation)}} %Command to signal missing citations



%------------------------------------------------------------------%

\author{Lucas Paul Unterweger}
\title{On the Duality of Frequentist Point Estimates and Bayesian Shrinkage Priors: An Extension based on the Triple-Gamma-Prior}


\begin{document}

\begin{titlepage}
\center
\vfill
\includegraphics[scale=0.1]{WU.png}
\vfill
\begin{tabular}[t]{lc}
Student-ID:  & 11913169 \\
Degree Program: & 
Master of Science in Economics (Science Track) \\
Examiner: & Peter Knaus, PhD \\
Submission date: & TBA \\
\end{tabular}
\vfill
{\large \textbf{Triple-Gamma-Regularization} \\
\normalsize \textit{A Flexible Non-Convex Regularization Penalty based on the Triple-Gamma-Prior}}

% Old Title
%\textit{or}\\ \textbf{On the Duality of Frequentist Point Estimates and Bayesian %Shrinkage Priors}\\
%\normalsize \textit{An Extension based on the Triple-Gamma-Prior}}

\vfill
by\\ \vspace{3mm}
{\Large Lucas Unterweger \href{https://github.com/therealLucasPaul}{\includegraphics[scale=0.01]{GitHub.png}}}\\
(Student-ID: 11913169)\\
\vfill

\thispagestyle{empty}
\pagebreak
\end{titlepage}
\newcounter{savepage}
\pagenumbering{roman}
\thispagestyle{empty}

\begin{abstract}
Lorem ipsum...
\end{abstract}

\clearpage

\section*{Acknowledgements}
\thispagestyle{empty}

I like to acknowledge ...

\clearpage

\thispagestyle{plain}
\tableofcontents
\pagebreak
\setcounter{savepage}{\arabic{page}}
\pagenumbering{arabic}

\pagebreak
%----- Introduction -----
\section{Introduction}
Willam of Ockham, born in Ockham, Surrey, probably lived between 1287 and 1348 and is nowadays recognized as a pre-eminent philosopher of the middle ages. Although     his name itself is no common knowledge, a principle carrying his name is: \textit{Ockham's Razor}. Interestingly, the main formulation of the principle (\textit{Entia non sunt multiplicanda praeter necessitatem} [plurality should not be posited without necessity]) can not be traced back to Ockham directly, but variations of it can be found in Ockham's writings. Since then nonetheless, the principle has long been used by statisticians and other researchers as a a scientific credo to capture the notion that "the simpler of two explanations is to be preferred" \parencite{Lazar2010}.  

%----- Theoretical Section
\newpage
\section{Theoretical Section}\label{sec:theorysuper}
Generally speaking, a major part of statistical learning deals with trying to describe a certain output variable $Y$ with a set of input variables $X_1, \cdots, X_p$ by trying to find a functional form $f$ which uses the given information in the inputs and - ideally - describes the hidden relationship between $Y$ and the inputs $X_i$ as accurately as possible. However, it comes as no surprise that the functional form $f$ depends on the statistical problem at hand. What type of data has been collected? Are we assuming a linear or non-linear relationship between the predictors? How much data is available and can its quality be guaranteed? But more importantly, it is necessary to ask the question whether the goal of the statistical learning method is \textit{inference} or \textit{prediction}.\\

The first of these two goals - \textit{inference} - aims at understanding the relationship that may or may not exist between the input variables $X_i$ and the output $Y$. Especially applied sciences like Economics, Psychology and Medicine often try to find a (causal) relationship within their theoretical framework to evaluate a policy, a medication or a new form of therapy. Linear models for example often provide a simple and straightforward framework which provide the scientist with interpretable effects. \textit{Prediction} on the other hand aims at forecasting the output variable $Y$ as accurately as possible and using every bit of information that is available, but disregards interpretability. \parencite{ESL,21} These goals can be summarized by viewing it as a decision between \textit{prediction accuracy} and \textit{model interpretability}, which has thoroughly been explained by \textcite{ESL}.\\

For the purpose of this thesis, the following chapters will restrict itself to the case of linear models, hence where assume that the functional form $f$ is linear in its inputs. The commonly known \textit{least squares estimator} is one way of estimating such a functional form and due to its many desirable properties has gained popularity in various  scientific fields. 

\subsection{Model Complexity and the Problem of Under- and Overfitting}

\begin{lightbluebox}
- BIAS VARIANCE TRADE-OFF
- DIMENSIONALITY PROBLEM

General problem: Fit vs. Complexity. What to choose and where is the optimal balance?
\end{lightbluebox}

In general, a linear regression model fitted with least squares on a sufficient amount of data points ($n \gg p$) will produce estimates which both have low bias and low variance and thus tends to perform well out of sample. However, problems arise with this approach arise when the sample size decreases, because the variance in the estimates will increase. The main idea behind this can be easily be visualized by trying to fit a polynomial curve on a two dimensional space and then altering the available sample for polynomials of different order. 
\begin{figure}
\centering
\includegraphics[scale=0.5]{PolynomialFit_Plot.png}
\caption{1st and 8th order polynomial fit to data \\ (\textcolor{green1}{Green} \& \textcolor{blue1}{Blue} Lines fitted with red data point; \textcolor{orange1}{Orange} \& \textcolor{red1}{Red} Lines fitted without).}
\label{fig:polyfit}
\end{figure}

Such a plot can be found in figure \ref{fig:polyfit}. Here, the black points represent the main data sample plotted on a two-dimensional $Y$-$X$-space. The red dot plotted at $(5,4)$ represents the additional data point which is used to alter the respective sample. Polynomials of specific orders can now be used to emulate certain levels of model complexity. For example, a first order polynomial has two coefficients to estimate: the intercept $\beta_0$ and the slope $\beta_1$. A polynomial of order of order eight has nine coefficients to estimate. What can now be seen in figure \ref{fig:polyfit} is the change in estimates if we include/exclude the additional red data point. A first-order polynomial needs at least two data points and as $n=9 > p = 2$, the overall fit of the polynomial doesn't change drastically with the additional tenth data point. In the case of the eighth-order polynomial, we need at least nine data points to create a fit as we have nine predictors (one intercept and one coefficient for each of the eight powers). In this case $n=p$ and adding another data point drastically changes the estimates coefficients, which means that the model won't perform well in out-of-sample scenarios. This emphasizes the importance a sufficient sample size when fitting linear models. And this leads us to the core field of study of this thesis: What are ways to improve the generalization of a linear model in scenarios where the sample size is not sufficiently large enough?


 	
\begin{lightbluebox}

- WAYS TO BATTLE COMPLEXITY

Solutions (find source, seen in LMU course): (1) More data (Not always feasible) (2) Better data (Same issue) (3) Reduce Model Complexity (Regularization) (4) Less aggressive optimization (Maybe short review; Idea: Stop optimization when generalization error degrades)
\end{lightbluebox}


\subsubsection{Regularization}

Focusing on option three to battle model complexity, regularization methods have been studied thoroughly since the 1990s. However, the emergence of data science - and especially machine learning - as a standalone field of study has led to a broader meaning of the term \textit{regularization}. This phenomenon has been discussed by \textcite{Taxonomy2017}, where the authors establish a taxonomy to distinguish between multiple different definitions. In the traditional sense, as can be seen in \textcite[167--170]{ESL2009}, \textit{regularization} refers to a general class of problems of the form
\[
\underset{f \in \mathcal{H}}{\min}\left\{\sum_{i=1}^N L(y_i, f(x_i)) + \lambda{J}(f)\right\}
\]
where $L(.)$ refers to a loss function defined as some function of the true values and the predicted values and $J(f)$ is penalty based on the chosen functional from a space of functions $\mathcal{H}$. In the context of penalized linear regression, this is equivalent to finding the set of risk minimizing coefficients $\hat{\beta}$ from the set of all possible combinations of coefficients $\boldsymbol{\beta}$. \MC Thus, resulting in the general class of regularization problems of the form:
\[
\underset{\beta \in \boldsymbol{\beta}}{\min}\left\{\sum_{i=1}^N L(y_i, f_\beta(x_i)) + \lambda{J}(\beta)\right\}
\]
where $f_\beta(x_i)$ is a linear function of the inputs $x_i$ parametrized by the coefficients $\beta$. Hence, in this setting, regularization deals with penalizing the risk function based on the value of the chosen set of coefficients. \\
However, this only describes a subset of \textit{regularization} methods as stated by \textcite{Taxonomy2017}. The authors use a more general definition of regularization:\\

\textbf{Defintion 1. Regularization} is any supplementary technique that aims at making the model generalize better, i.e. produce better results on the test set.\\

Building on that, they split up the majority of \textit{regularization} methods into  (1) methods applied to the data set like transformations or modifications of the inputs, (2) methods altering the selected model family , (3) methods applied to the error/loss function $L(y_i, f_\beta(x_i))$ , (4) methods applied to regularization/penalty term as described above and (5) alterations of the optimization procedure itself. 

\begin{lightbluebox}
Vielleicht noch genauer auf die Taxonomy eingehen? 
\end{lightbluebox} 

Unsurprisingly, this thesis is concerned with the fourth group of regularization methods, which add a penalty/regularizer term $J(\beta)$ into the risk function, but before advancing to literature that deals with this kind of problem, it is necessary to establish a terminology which will be used throughout this thesis. \textbf{TODO}

Let $\mathcal{D}$ be a training data set with $n \in \mathbb{N}$ observations, where every  consists of a target variable $y_i \in \mathbb{R}$ along with a number of corresponding inputs $x_i \in \mathbb{R}$. Given a linear function $f_\beta(x_i)$ of the inputs parametrized by coefficients $\beta \in \boldsymbol{\beta}$, $L(y_i f_\beta(x_i))$ is the \textbf{Loss} function measuring the discrepancy between the actual target $y_i$ and the output of the linear function $f_\beta(x_i)$. According to \textcite{Vapnik1991}, the \textbf{Empirical Risk Functional} is then 
\[
R_{emp}(\beta) = \frac{1}{n} \sum_{i=1}^n L(y_i, f_\beta(x_i)).
\]
\textbf{Regularization} in this thesis' context refers then to adding some penalty function $J(\beta)$ dependent on the set of parameters $\beta$, multiplied by some weighting parameter $\lambda$, to the empirical risk functional $R_{emp}(\beta)$. Thus, $R_{reg} = R_{emp}(\beta) + \lambda\cdot J(\beta)$. This results in the overall optimization problem
\begin{align*}
&\underset{\beta \in \boldsymbol{\beta}}{\argmin}\left\{R_{reg}(\beta)\right\}\\
&=\underset{\beta \in \boldsymbol{\beta}}{\argmin}\left\{R_{emp}(\beta) + \lambda\cdot J(\beta)\right\}\\
&=\underset{\beta \in \boldsymbol{\beta}}{\argmin}\left\{\sum_{i=1}^n L(y_i, f_\beta(x_i)) + \lambda\cdot J(\beta)\right\}
\end{align*}

It is important to note here that as $J(\beta)$ is only a function of the coefficients $\beta$ and neither the targets $y_i$ nor the inputs $x_i$. It only affects the generalization error of the model, not the training error given by the empirical risk functional $R_{emp}(\beta)$.

\begin{lightbluebox}
Ad Solution (3) because it is relatively easy to implement: Simple approach => Start with simplest model and iteratively add one feature OR iteratively get rid of feature by feature. (Problem: Very arbitrary and hard to reasonably do) Thus, adjust risk function minimization. Important: Regularization adjusts generalization error, not training error!

Short break: Talk about Taxonomy by Kukacka et al 2017 on the differing use of the term "regularization" 

What is needed: 
\[
R_{reg}(\beta) = R_{emp}(\beta)+ \lambda \cdot J(\beta)
\]
where $R_{emp}(\beta) = \sum_i^n L(y_i, f(x_i, \beta))$. Note: Regularization term does not depend on data, just on parametrization. $\lambda$ controls the strength of regularization. Thus, $\lambda = 0$ means simple MSE optimization and $\lambda \to \infty$ chooses simplest model. As $\lambda$ is set manually, this also bears some problems. However, typical solution is cross-validation. 

\textbf{Literature}
\parencite{Taxonomy2017}
\end{lightbluebox}

\subsection{Bayesian View on Battling Model Complexity using Shrinkage Priors}
Explain how Bayesians battle model complexity. 

\newpage

%----- Literature Review -----
\section{Literature Review}\label{sec:litreview}

The concept of a penalized regression has been around for quite some time and been studied widely in various fields of scientific research. Arguably, this methodological approach to penalized regression started with the publication of two pieces of literature published by Arthur Hoerl and Robert Kennard in 1970 \parencite{HoerlKennard1970a, HoerlKennard1970b}. With these two papers the authors introduced the widely known \textit{Ridge Regression}, which has been developed from the previously known concept of Ridge analysis. In its core, the authors were trying to tackle the problem of high variances of the regression coefficients in high-dimensional problem settings. This shrinkage estimator, which uses the squared coefficient as a penalty term, "attempt[s] to shrink the coefficients to
reduce these variances, while adding some bias." \parencite{Hoerl2020} This closely resembles the previously discussed issue of the \textit{Bias-Variance-Tradeoff}, which has been discussed in the \textit{Under- and Overfitting} chapter in section \ref{sec:theorysuper} (Roger W. Hoerl, Arthur Hoerl's son, published a historical overview of the development of the concept of \textit{Ridge Regression} in 2020 \parencite{Hoerl2020}). The closed from solution of the Ridge estimator is given by
$$\hat{\beta}_{Ridge} = \left(X^TX+\lambda I\right)^{-1}X^Ty$$,
which adjusts the OLS estimator by shifting the main diagonal entries of the design matrix by $\lambda$ ($\lambda \geq 0$). It can be shown that this closed form estimator is equivalent to a Lagrangian problem of the following form \parencite{VanWieringen2015}:
$$\hat{\beta}_{Ridge}(\lambda) = \underset{\beta}{\argmin}\left\{\norm{y-X\beta}_2^2 + \lambda\norm{\beta}_2^2\right\}$$
This resembles an example of the above defined regularization framework with squared residual loss and a penalty term of the form $\norm{\beta}_2^2$, which only depends on the parameter $\beta$. In case of $\lambda$ being equal to zero, this reduces to the \textit{maximum likelihood (ML) estimator}. 

The publications of Arthur Hoerl and Robert Kennard have led to further advancements, although it took more than 25 years, in shrinkage estimation or related concepts. One concept which is almost as famous \textit{Ridge Regression} is the \textit{Least Absolute Shrinkage and Selection Operator}, more commonly know as \textit{LASSO}, developed by \textcite{Tibishirani1996}. He argues that the two at the time most prominent shrinkage methods - Ridge and Subset Selection - both have their drawbacks. Ridge regression on the one hand is an optimization problem which continuously shrinks coefficients towards zero, but doesn't select them in a discrete sense, which makes it hard to interpret these models. Subset Selection on the other hand chooses variables in a discrete sense - a variables either stays within the model or it doesn't - and thus creates easily interpretable models, but "[s]mall changes in the data can result in very different models being selected and this can reduce its prediction accuracy." \parencite{Tibishirani1996} \textit{LASSO} is trying to combine both methods' advantages by using $\norm{\beta}_1$ as a penalty term.\\
\begin{lightbluebox}
Bit more on LASSO?
\end{lightbluebox}

\textit{LASSO} and to some extend \textit{Ridge} can be viewed as a special case of a $l_p$-norm regularization with corresponding values for $p$ ($p = 1$ for LASSO and $p=2$ for Ridge) \parencite{FrankFriedman1993}. 

$$\norm{\beta}_p =  \left(\sum_{i=1}^p|\beta_i|^p\right)^{1/p}$$

Work published by researchers in the nineties, like the previously mentioned \textcite{FrankFriedman1993} or \textcite{Fu1998}, as well as more recent literature like \textcite{WangEtAl2020} have repeatedly shown there is no go-to-method to tackle regularization problems, as the effectiveness of a specific approach highly depends on the data siuation at hand. Due to this particular situation in the literature, several other methods have been proposed in the recent years and decades. An approach combining \textit{Ridge} and \textit{LASSO} two methods is called \textit{Elastic Net} regularization and has been developed by \textcite{ZouHastie2005}. The authors there elaborate on some of the shortcomings of the LASSO method. For example, in a special case where there are more predictors $p$ than data points $n$ ($p > n$), LASSO only selects up to $n$ variables due to the nature of the convex optimization problem. Should several of the included variables be highly \textit{pairwise} correlated with each other, \textit{LASSO} tends to only select on of these variables. In its core, \textit{Elastic Net Regularization} linearly combines the penalty terms of \textit{Ridge} and \textit{LASSO} regularization, yielding a loss function of the form:\\
$$J(\beta) = \lambda_1 \norm{\beta}_2^2 + \lambda_2 \norm{\beta}_1$$
The authors have	 shown that, especially when it comes to encouraging the aforementioned grouping effects, \textit{Elastic Net} tends to perform better than the \textit{LASSO}.\\

Recent years however have opened up a new subfield of approaches to regularization. Methods like \textit{LASSO, Bridge} \parencite{FrankFriedman1993}, \textit{Ridge} and \textit{Elastic Net}\footnote{Elastic Net, due to its mathematical definition, can be view as a generalization of \textit{Ridge} and \textit{LASSO}.} are convex functions in its parameters and are thus usually classified as \textit{Convex Regularization Penalties}. Recently, the literature has shifted towards penalties which are non-convex functions in its parameters, usually called \textit{Non-Convex Regularization Penalties}\footnote{Note: They are called \textit{non-convex} penalties and not \textit{concave} penalties, because non-convexity does not necessarily imply concavity.}. One recent example of such a penalty includes \textcite{JohnVettamWu2022}, who proposed a penalty structure called \textit{Gaussian penalty} and which is based on a Gaussian-like function by using $J(\beta) = 1 - e^{-\kappa\beta^2}$. Another method proposed by \textcite{WangZhu2016} is called the \textit{Atan penalty} - or \textit{Arctan} penalty - an makes use of the favourable properties of the \textit{Arctan} function by using the penalty $J(\beta, \gamma) = (\gamma + \frac{2}{\pi})\arctan(\frac{|\beta|}{\gamma})$.


\begin{lightbluebox}
Use tree to classify regularization methods.
\end{lightbluebox}
\begin{center}
\begin{tikzpicture}[
    level 1/.style={sibling distance=5.2cm},
    level 2/.style={sibling distance=2.6cm},
    level 3/.style={sibling distance=2.6cm, level distance=2.5cm},
    every node/.style={draw, rectangle, align=center}
]
\node {Classification of \\ Penalty Terms}
    child { node {Behaviour at Zero}
        child { node {Singularity} child { node {TG, \\ MCP,\\ SCAD, \\ LAPLACE, \\ LASSO}}}
        child { node {Differentiable} child { node {Gaussian,\\ Ridge}}}
    }
    child { node {(Non-)Convexity}
        child { node {Convex} child { node {LASSO, \\ Ridge}}}
        child { node {Concave} child { node {TG,\\ Gaussian, \\ Arctan}}}
    }
    child { node {Behaviour at Limits}
        child { node {Divergent} child { node {TG,\\ LASSO, \\ Ridge}}}
        child { node {Convergent} child { node {Arctan, \\ Gaussian}}}
    };
\end{tikzpicture}  

\end{center}

Vielleicht als Tabelle mit drei Spalten.

\subsection{Duality of the Ridge Regression}

The main motivation for this thesis stems from the striking duality that exists between Bayesian shrinkage priors and approaches to regularization. Both approaches aim at tackeling the issue of model complexity and overfitting by making it necessary for the data to be more convincing that the value of an estimate is statistically significant differnt from zero. Bayesian statistics use specific prior distributions with a usually a lot of mass around zero and heavy tails. Popular examples...  

\subsection{Triple-Gamma-Prior by \textcite{TGP2020}}
A new development in the are of shrinkage priors has been made by \textcite{TGP2020}

%----- Model Setup and Derivation
\newpage
\section{Model Setup and Derivation}
Coming to the theoretical framework of the \textit{triple-gamma-regularization}, let's assume we have a response variable $y$ and $p$ predictors along with $n$ data points. More formally, let $y=[y_1  \quad y_2 \cdots y_n]^T$ and $x_i = [x_{i1} \quad x_{i2} \cdots x_{in}]^T$ with $\forall i\in \{1,...,n\}: y_i, x_i \in \mathbb{R}$. Here, $x_i$ is the $i$-th predictor, thus resulting in the design matrix $X = [x_1 \quad x_2 \cdots x_p]$. \\

Starting from the Bayesian framework, the standard linear regression model is given by
\[
y_i = x_i^T\cdot \beta + \varepsilon_i \quad i\in \{1,...,n\} 
\]
with the assumed distribution of $\varepsilon_i \sim N(0,\sigma^2)$. Thus it follows that $y\sim N_n(X\beta,\sigma^2I)$. The posterior distribution of the parameter vector $\beta$, according to Bayes' Rule, is then proportional to the product of the likelihood of the data and the prior distribution, which can be seen in equation \ref{eq:bayes}.
\begin{equation}\label{eq:bayes}
p(\beta|y, X, \sigma^2) \propto \mathcal{L}(y|\beta, \sigma^2, X)\times p(\beta)
\end{equation}	

As stated above, each data point $y_i$ is assumed to be identically and independently drawn from a normal distribution with mean $X\beta$ and variance $\sigma_i^2$, thus:
\begin{align}
\mathcal{L}(\mathbf{y}|\beta, \sigma^2, X) 	&= \prod_i^n p(y_i|\beta, \sigma^2, X_i) \nonumber\\
						&= \prod_i^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{1}{2}\left(\frac{x_i-\mu}{\sigma}\right)^2\right) \nonumber\\
						&= \frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left(-\frac{1}{2\sigma^2}(\mathbf{y}-\mathbf{X}\beta)^\top(\mathbf{y}-\mathbf{X}\beta)\right) \label{eq:likelihood}
\end{align}

The log of the likelihood function is then given by 
\begin{align*}
\log \mathcal{L}( \mathbf{y} |\beta, \sigma^2 , \mathbf{X}) &= \log \left( \frac{1}{(2\pi\sigma^2)^{n/2}} \exp\left(-\frac{1}{2\sigma^2} (\mathbf{y} - \mathbf{X}\beta)^\top (\mathbf{y} - \mathbf{X}\beta)\right) \right) \\
&= \log \left( \frac{1}{(2\pi\sigma^2)^{n/2}} \right) + \log \exp\left(-\frac{1}{2\sigma^2} (\mathbf{y} - \mathbf{X}\beta)^\top (\mathbf{y} - \mathbf{X}\beta)\right) \\
&= -\frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} (\mathbf{y} - \mathbf{X}\beta)^\top (\mathbf{y} - \mathbf{X}\beta)\\
&\propto - \frac{1}{2\sigma^2} (\mathbf{y} - \mathbf{X}\beta)^\top (\mathbf{y} - \mathbf{X}\beta) = - \frac{1}{2\sigma^2}\norm{\mathbf{y} - \mathbf{X}\beta}_2^2\\
\end{align*}

The marginal prior distribution for the parameter vector $\beta$ stems from the Triple-Gamma-Prior constructed in \textcite{TGP2020} given in Theorem 1 (a) and is given by
\begin{align*}
p(\sqrt{\beta_j}|\phi^\xi, a^\xi, c^\xi) &= \frac{\Gamma(c^\xi + \frac{1}{2})}{\sqrt{2\pi \phi^\xi}\cdot B(a^\xi, c^\xi)}\cdot U\left(c^\xi + \frac{1}{2}, \frac{3}{2}-a^\xi, \frac{\beta_j}{2\phi^\xi}\right)\\
&\propto U\left(c^\xi + \frac{1}{2}, \frac{3}{2}-a^\xi, \frac{\beta_j}{2\phi^\xi}\right)
\end{align*}
Here, $U(a,b,z)$ refers the confluent hyper-geometric function of the second kind which was introduced by \textcite{Tricomi1947}. As this prior is specified for the parameter $\sqrt{\beta_j}$, we transform the prior by squaring the parameter to gain
\begin{align*}
p(\beta_j|\phi^\xi, a^\xi, c^\xi) &\propto U\left(c^\xi + \frac{1}{2}, \frac{3}{2}-a^\xi, \frac{\beta_j^2}{2\phi^\xi}\right)
\end{align*}
Now, assuming that the parameters are independent a priori, the prior distribution is given by
\begin{align}
p(\beta) 	&= \prod_j^p p(\beta_j) \nonumber\\
			&= \prod_j^p p(\beta_j|\phi^\xi, a^\xi, c^\xi) \nonumber\\
			&\propto \prod_j^p U\left(c^\xi + \frac{1}{2}, \frac{3}{2}-a^\xi, \frac{\beta_j^2}{2\phi^\xi}\right) \nonumber\\
			&= \prod_j^p \frac{1}{\Gamma(c^\xi + \frac{1}{2})}\int_0^\infty e^{-(\frac{\beta_j^2}{2\phi^\xi})t}t^{c^\xi + \frac{1}{2}-1}(1+t)^{\frac{3}{2}-a^\xi-c^\xi + \frac{1}{2}-1}dt \nonumber\\
			&\propto \prod_j^p \int_0^\infty \exp\left(-\frac{\beta_j^2}{2\phi^\xi}t\right)t^{c^\xi - \frac{1}{2}}(1+t)^{1-a^\xi-c^\xi} dt \label{eq:prior}
\end{align}

Here, in line 1 the assumption of independence between the parameters has been used to describe the distribution of the parameter vector as the product of its individual parameter distributions. In line 2, the marginal prior from \textcite{TGP2020} has been used as the prior distribution for each individual parameter $\beta_j$. In line 3, scaling parameters have been removed by using the proportionality assumption. The last two lines of the derivation insert the integral representation of the confluent hyper-geometric function of the second kind, $U(a,b,z)$, which is valid in the case of a positive real part for the first parameter ($\mathfrak{Re}(a) > 0$) and again apply proportionality.\\

Taking the $\log$ of the prior distribution and using the properties of the logarithmic function yields the general result
\[
\log(p(\beta))=\log(\prod_j^p p(\beta_j|\phi^\xi, a^\xi, c^\xi))=\sum_j^p \log(p(\beta_j|\phi^\xi, a^\xi, c^\xi))
\]

A common approach to estimation in regularization settings is the \textit{maximum a posteriori probability (MAP)} estimator \MC ,which is defined as
\[
\hat{\beta}_{MAP}(x) = \underset{\beta \in \mathbb{R}^p}{\argmax}\left\{f(x|\beta)g(\beta)\right\}
\]
where $f(x|\beta)$ describes the the probability density function of a variable $x$, which is parametrized by the parameter vector $\beta$. The second function $g(\beta)$ incorporates our prior information about the parameter vector $\beta$ into the optimization problem.\\  

Returning to our specific problem at hand, the posterior distribution of our parameter vector $\mathbf{\beta}$ can be retrieved by applying Bayes' theorem and the previously gained results in equations \ref{eq:prior} and \ref{eq:likelihood}. Thus, the posterior distribution of the parameter vector $\beta$ is proportional to
\begin{align}
p(\beta|y, X, \sigma^2) 	&\propto p(y|X,\beta,\sigma)\times p(\beta) \nonumber\\
						&\propto \frac{1}{(2\pi\sigma^2)^{n/2}}e^{-\frac{1}{2\sigma^2}(y-X\beta)^T(y-X\beta)} \times \prod_j^p U\left(c^\xi + \frac{1}{2}, \frac{3}{2}-a^\xi, \frac{\beta_j^2}{2\phi^\xi}\right)\label{eq:posterior}
\end{align} 

Making use of the monotonicity of the logarithmic function and seeing that it is easier to optimize the log-posterior, Taking the log of the posterior probability distribution, we take the log of result \ref{eq:posterior}. 
\begin{align}
\log(\beta| X, y, \sigma^2) 	&= \log\left(\frac{1}{(2\pi\sigma^2)^{n/2}}\right) - \frac{1}{2\sigma^2}\norm{\mathbf{y} - \mathbf{X}\beta}_2^2 + \log\left(\prod_j^p U\left(c^\xi + \frac{1}{2}, \frac{3}{2}-a^\xi, \frac{\beta_j^2}{2\phi^\xi}\right)\right)\nonumber\\
							&= \log\left(\frac{1}{(2\pi\sigma^2)^{n/2}}\right) - \frac{1}{2\sigma^2}\norm{\mathbf{y} - \mathbf{X}\beta^2}_2^2 + \sum_j^p \log\left(U\left(c^\xi + \frac{1}{2}, \frac{3}{2}-a^\xi, \frac{\beta_j}{2\phi^\xi}\right)\right)\nonumber\\
							&\propto -\frac{1}{2\sigma^2}\norm{\mathbf{y} - \mathbf{X}\beta}_2^2 + \sum_j^p \log\left(U\left(c^\xi + \frac{1}{2}, \frac{3}{2}-a^\xi, \frac{\beta_j^2}{2\phi^\xi}\right)\right)\nonumber\\
\end{align}

To align with the general specification structure of regularization problems, which can be seen from equation \textbf{missEQ}, a parameter $\lambda$ will be multiplicatively added in front of the penalty term, which makes it possible to adjust the strength of the influence that the penalty has on the chosen parameters. By minimizing the negative log-posterior adjusted with $\lambda$, we can retrieve the \textit{maximum a posteriori probability (MAP)} estimator using \textbf{Triple-Gamma-Regularization}:
\begin{equation}\label{eq:tgr}
\hat{\beta}_{MAP} = \underset{\beta \in \mathbb{R}^p}{\argmin} \left(\frac{1}{2\sigma^2}\norm{\mathbf{y} - \mathbf{X}\beta}_2^2 + \lambda \sum_j^p -\log\left(U\left(c^\xi + \frac{1}{2}, \frac{3}{2}-a^\xi, \frac{\beta_j^2}{2\phi^\xi}\right)\right)\right) 
\end{equation}

\newpage
\subsection{Varying the Hyperparameters}
After closer inspection of equation \ref{eq:tgr}, it can easily be seen that this resembles the general penalized regression already seen in \textcite{ESLpage398} and in section \ref{sec:theorysuper} as $R(\beta) + \lambda\cdot J(\beta)$. The first term, also called the empirical loss in machine learning literature, is the widely known residual sum of squares:
\[
R(\beta)=\frac{1}{2\sigma^2}\norm{\mathbf{y} - \mathbf{X}\beta}_2^2
\] 
The second part of the optimization problem can be viewed as a penalty imposed on the total risk based on the size of the estimates:
\[
J_{TG}(\beta) = \sum_j^p -\log\left(U\left(c^\xi + \frac{1}{2}, \frac{3}{2}-a^\xi, \frac{\beta_j^2}{2\phi^\xi}\right)\right)
\]
In contrast to the \textit{LASSO} penalty, which uses the the absolute value of the coefficient, or the \textit{Ridge} penalty, which uses the square of the estimate, this penalty derived from \textcite{TGP2020} is based on the $\log$ of the confluent hyper-geometric of the second kind. Notably, this penalty term has three additional hyper-parameters: $c^\xi$, $a^\xi$ and $\kappa_B$ as $\phi^\xi = (2c^\xi)/(\kappa^2_B a^\xi)$. Here, the restrictions $a^\xi>0.5$ and $0 < c^\xi < \infty$ are necessary to ensure that the penalty for a $\beta_j$ being equal to zero remains finite and not diverges to negative infinity at zero. This results, which has already been presented and proven as part of Theorem 2 in \textcite[5--6]{TGP2020}, ensures that the negative $\log$ of the hypergeometric function remains finite and thus does not produce parameter estimates which are zero for every variables. (\textbf{BESSER SCHREIBEN?})\\
\begin{lightbluebox}
Write more to fill this page
\end{lightbluebox}
\newpage

\textit{Variations of the Hyperparameter $a^\xi$}\\

The first hyper-parameter which can be adjusted is $a^\xi$. A plot with a set of different values for $a^\xi$ can be found in figure \ref{fig:VariationInA}. As already mentioned earlier, the necessary restriction for for this hyper-parameter is that it has to be strictly greater than $0.5$ to guarantee the finiteness of the penalty. To demonstrate the effects of changes in $a^\xi$, the other parameters have been set to $c^\xi=0.1$ and $\kappa_B=2$. It can easily be seen from the figure that $a^\xi$ steers the sharpness of the penalty in small neighbourhoods around $\beta=0$. As $a^\xi$ increases, the penalty because smoother at $\beta=0$ with it eventually converging a \textit{Gaussian Penalty} like behaviour. From a modelling perspective, this opens up the possibility of steering the degree of variable selection the penalty performs. Nonetheless, the overall structure of the penalty in the tails does not change systematically apart from a parallel shift, which can be readjusted by specifying a different weighting parameter $\lambda$ or a different value for $\kappa_B$ (more on effect of $\kappa_B$ on the penalty can be found later in this chapter).\\ 

\begin{figure}[!h]
\centering
\includegraphics[scale=0.75]{../02_simulation/021_simulation_figures/TGPenalty_ChangeInA.png}
\caption{Triple-Gamma-Penalty using different values of $a^\xi$}
\label{fig:VariationInA}
\end{figure}

Seeing this, it is apparent that a value of $a^\xi$ close but strictly larger that $0.5$ mimics the behaviour of the \textit{Arctan Penalty} by \textcite{WangZhu2016} in small neighbourhoods of $\beta=0$. Similar, large positive values for $a^\xi$ lead to a \textit{Gaussian Penalty} like behaviour in small neighbourhoods of $\beta = 0$ as recently proposed by \textcite{JohnVettamWu2022}.\\
\newpage

\textit{Variations of the Hyperparameter $c^\xi$}\\

In contrast to the hyperparameter $a^\xi$, which mainly affects the behaviour at and around $\beta=0$, changes in $c^\xi$ mainly affect the behaviour in the tails. However, the effect that a change in $c^\xi$ has on the penalty structure can be split up in two rough subsets of $(0,\infty)$. The effect of the first subset of values for $c^\xi$ which are strictly greater than $0$ but less or equal than $0.1$ can be found in figure \ref{fig:ChangeInC_SmallValues} (as already mentioned before, by definition, $c^\xi$ has to be strictly greater than zero: $c^\xi > 0$). Here, it can be seen that as the values for $c^\xi$ become smaller, a shifting effect takes place which generally does not influence the overall structure of the penalty, but increases the amount of penalty which is added to the risk function for $\beta$-values which are different from zero (In a sense, this has a similar effect to changes in $\kappa_B$, which will be explained later). Or, to put it differently, with values of $c^\xi$ closer to zero, the data has be become even more convincing that the value is significantly different from zero.\\


\begin{figure}[!h]
\centering
\includegraphics[scale=0.75]{../02_simulation/021_simulation_figures/TGPenalty_ChangeInC_Part2.png}
\caption{Triple-Gamma-Penalty using different values of $c^\xi$ with $0 < c^\xi \leq 0.1$}
\label{fig:ChangeInC_SmallValues}
\end{figure}

However, the more interesting effect that a change in $c^\xi$ has on the penalty structure can be seen for values of $c^\xi$ that are greater than $0.1$. In figure \ref{fig:ChangeInC_LargeValues}, a plot can be found with the Triple-Gamma-Penalty for larger values of $c^\xi$. Again, starting from the baseline with $c^\xi = 0.1$, higher values for this hyper-parameter mainly change the behaviour of the penalty in the tails. A result that has already been shown by \textcite{TGP2020} in Table 1, where multiple different hyper-parameter settings are presented, is that with an increasing value for $c^\xi$ and with $a^\xi = 1$ \textbf{as well as kappaB = ??}, the Triple-Gamma-Prior converges to a \textit{LASSO} like shrinkage behaviour. A property which carries over to the proposed regularization setting when using the proposed hyper-parameter values, thus showing that the Triple-Gamma-Penalty can be used both as a non-convex penalty as well as a \textit{LASSO} penalty, creating increased flexibility in modelling approaches. 

\begin{figure}[!h]
\centering
\includegraphics[scale=0.75]{../02_simulation/021_simulation_figures/TGPenalty_ChangeInC_Part1.png}
\caption{Triple-Gamma-Penalty using different values of $c^\xi$ with $c^\xi \geq 0.1$}
\label{fig:ChangeInC_LargeValues}
\end{figure}

\textit{Variations of the Hyperparameter $\kappa_B$}\\

The third and final hyper-parameter $\kappa_B$ enters the Triple-Gamma-Penalty $J_{TG}(\beta)$ as part of $\phi^\xi = \frac{2c^\xi}{\kappa_B^2 a^\xi}$ as can be seen from equation \ref{eq:tgr}. Notably, $\kappa_B$ is squared and thus only the absolute value of $\kappa_B$, $|\kappa_B|$, influences the structure of the penalty. The overall third function value is defined as $\frac{\beta_j^2}{2\phi^\xi}$ and by plugging in $\phi^\xi$ we get $\frac{\beta_j^2\kappa_ B^2a^\xi}{4c^\xi}$, it can be seen that a value of $\kappa_B = 0$ leads to the entire parameter value being zero for all values of $\beta_j$. Hence, a change in $\beta_j$ won't influence the penalty and furthermore won't have an influence on the overall risk minimization problem, the result being that the optimal set of parameters will only depend on the chosen loss function.\\

For all values of $\kappa_B \neq 0$, the value of the hyper-parameter will influence the penalty structure. A plot with several different values for the absolute value of $\kappa_B$ can be found in figure \ref{fig:TGPenalty_ChangeInKappa}.

\begin{figure}[t]
\centering
\includegraphics[scale=0.75]{../02_simulation/021_simulation_figures/TGPenalty_ChangeInKappa.png}
\caption{Triple-Gamma-Penalty using different values of $\kappa_B$}
\label{fig:TGPenalty_ChangeInKappa}
\end{figure}

\newpage
%---- Table Summarizing the Effects of Changes in the Hyperparameters
\begin{landscape}
\begin{table}[!h]
\begin{center}
\begin{tabular}{lcccccc}\toprule
		& \multicolumn{2}{l}{Change} & \multicolumn{3}{c}{Mathematical Properties}\\
		\cmidrule(r{4pt}){2-3} \cmidrule(l){4-5}
Variable     	& \textbf{Positive} Change  & \textbf{Negative} Change & Defined Range & Misc.\\\midrule
$a^\xi$  	& \makecell{Shifting towards\\ \textit{Gaussian}-like behaviour\\ at $\beta = 0$} & \makecell{Shifting towards singularity\\ at $\beta = 0$; Higher\\ immediate penalty for coefficients\\ $\beta \neq 0$} & $(0.5, \infty)$ & Expl.\\
$c^\xi$ 		& \makecell{Generally, convergence towards \\ convexity and, given certain \\ settings for $a^\xi$ and\\ $\kappa_B$, LASSO. Higher values\\ increase the additional penalty\\ for higher absolute values of $\beta$.} &  \makecell{For values smaller \\ than $0.1$, similar effect\\ to increase in $a^\xi$} & $(0, \infty)$ & Expl. \\
$\kappa_B$ 	& 15883 & 5.2e-8 & $(-\infty, \infty)/\{0\}$ & Expl. \\\bottomrule
\end{tabular}
\caption{Summary of the effects of changes in the hyperparameters $a^\xi$, $c^\xi$ and $\kappa_B$ on the penalty structure}
\label{tab:VariationsInHyperparameters}
\end{center}
\end{table}
\end{landscape}
\newpage

\subsection{Comparison to already existing Penalty Terms}\label{subsec:comparepenalties}

As already mentioned in section \ref{sec:litreview}, several other penalty terms have already been widely studied in the literature. Convex penalties like \textit{Ridge} \parencite{HoerlKennard1970a} or non-convex penalties like the \textit{Ar(c)tan} \parencite{WangZhu2016} and \textit{Gaussian} \parencite{JohnVettamWu2022} have managed to establish itself as prominent approaches to regularization. It is now certainly of interest to see how the \textit{Triple-Gamma} penalty compares to the established methods. Seeing that, due to its flexibility, there is not \textit{one} \textit{Triple-Gamma} penalty, three distinct hyper-parameter setting have been chosen to represent the proposed penalty term. 

\begin{figure}[!h]
\centering
\includegraphics[scale=0.75]{../02_simulation/021_simulation_figures/TGPenalty_Comparison.png}
\caption{Basic Representation of the Triple-Gamma-Penalty using Hyperparameters $c^\xi = 0.1, \kappa_B = 2, a^\xi = 0.75$ compared to \textit{LASSO} and \textit{Ridge} Penalties}
\label{fig:basicTGPen}
\end{figure}

\subsection{Restricted Strong Convexity (RSC) of the Triple-Gamma-Regularization}

\subsection{Alternative Specification (with +1 to mimic artan)}

As previously mentioned, the \textit{Arctan} penalty developed by \textcite{WangZhu2016} has a similar structure as the \textit{Triple-Gamma-Penalty} in small neighbourhoods near $\beta = 0$ when using specific hyper-parameters (see section \ref{subsec:comparepenalties}). The distinctive structural difference between these two penalties emerge when looking at the limits when  $\underset{\beta \to +/- \infty}{\lim J_{TG}(\beta)}$

\subsection{Approaches to Estimation}

%----- Simulation Section
\section{Simulation Section}
Use the earlier derivation, code up the functions and simulate data to check behaviour for different datasets. Compare to base OLS, Ridge and Lasso Regression?
\subsection{Computational Performance}
Talking about Gradient Descent Methods in more depth. Why Gradient Clipping. Maybe more modern estimation techniques? 

See how computation times change when increasing the size of the data set or when the number of parameter changes. => Use Stochastic Gradient Descent (SGD)
\subsection{Implementation as Python Package}

How to use it. 
Explanation of Functions. Input - Output Tables

%----- Small Applied Section
%\section{Small Applied Section}
%Here I am planning to apply the TGP using Peter's Bayesian Package and the self coded frequentist code on a small dataset.
\section{Possible Extensions and Criticism}
\section{Conclusion}

\newpage
\section{List of Figures}
\listoffigures

\newpage
\section{List of Tables}
\listoftables

\newpage
\section{References}

\pagebreak
\pagenumbering{roman}
\setcounter{page}{\thesavepage}
\pagestyle{plain}
\addcontentsline{toc}{section}{References}
%\bibliographystyle{apalike}
%\bibliography{ref.bib}
\printbibliography[]
\clearpage
\appendix
\end{document}
